[
  {
    "objectID": "GLM.html",
    "href": "GLM.html",
    "title": "Ch. 3: Generalized Linear Models",
    "section": "",
    "text": "Reading/working time: ~35 min.\nIn the previous chapters, we have learned how to conduct simulation-based power analyses for linear regressions with categorical and/or continuous predictor variables. In this chapter, we will turn to generalized linear models, which constitute a generalization of regular linear regressions. This class of statistical models additionally comprise more complex models such as logistic regression and poisson regression. However, as covering all of variants of the generalized linear models would exceed the scope of this workshop, we will only learn how to conduct a simulation-based power analysis for a logistic regression. Recall that a logistic regression is suitable for designs in which you predict a dichotomous outcome with a set of (continuous or categorical) predictors."
  },
  {
    "objectID": "GLM.html#lets-get-some-data-as-a-starting-point",
    "href": "GLM.html#lets-get-some-data-as-a-starting-point",
    "title": "Ch. 3: Generalized Linear Models",
    "section": "Let’s get some data as a starting point",
    "text": "Let’s get some data as a starting point\n\n# load the data\ndata(\"BtheB\", package = \"HSAUR\")\n\n# show which variables this data set includes\nstr(BtheB)\n\n'data.frame':   100 obs. of  8 variables:\n $ drug     : Factor w/ 2 levels \"No\",\"Yes\": 1 2 2 1 2 2 2 1 2 2 ...\n $ length   : Factor w/ 2 levels \"<6m\",\">6m\": 2 2 1 2 2 1 1 2 1 2 ...\n $ treatment: Factor w/ 2 levels \"TAU\",\"BtheB\": 1 2 1 2 2 2 1 1 2 2 ...\n $ bdi.pre  : num  29 32 25 21 26 7 17 20 18 20 ...\n $ bdi.2m   : num  2 16 20 17 23 0 7 20 13 5 ...\n $ bdi.4m   : num  2 24 NA 16 NA 0 7 21 14 5 ...\n $ bdi.6m   : num  NA 17 NA 10 NA 0 3 19 20 8 ...\n $ bdi.8m   : num  NA 20 NA 9 NA 0 7 13 11 12 ...\n\n\nThis data set compares the effects of two interventions for depression, that is a so-called “Beat the blues” intervention (BtheB) and a “treatment-as-usual” intervention (TAU). Note that in this chapter we will compare two active treatment conditions with each other – in the other chapters on linear models, we compare an active treatment with a passive waiting group.\nUnfortunately, this data set does not include any dichotomous variable we could use as an outcome measure. Therefore, we simply dichotomize one of the continuous outcome variables to create a categorical outcome artificially. More specifically, we will dichotomize the bdi.2m variable which contains a follow-up depression measure (i.e., the Beck Depression Inventory score) two months after the intervention.\n\n\n\n\n\n\nNote\n\n\n\nPlease note that we dichotomize this variable for didactic reasons only. From a statistical point of view, it is preferable to treat a continuous variable as such because dichotomizing leads to a loss of statistical information (Royston et al., 2006).\n\n\nLet’s start by dichotomizing the bdi.2m variable and storing the result in a new variable which we label bdi.2m.dicho. Here, we take 20 as our cut-off value, as BDIs of 20 or more refer to a moderate or severe depression. BDI values lower than 20, in turn, reflect a minimal or mild depression (see first chapter on linear models for more info). In our new bdi.2m.dicho variable, we code minimal/mild depression as “0” and moderate/severe depression as “1”.\n\n#dichotomize dv\nBtheB$bdi.2m.dicho <- ifelse(BtheB$bdi.2m < 20, 0, 1)\n\n#show frequencies\ntable(BtheB$bdi.2m.dicho, BtheB$treatment)\n\n   \n    TAU BtheB\n  0  21    37\n  1  24    15\n\n\nThe frequency table shows that there were less patients with moderate/severe depression in the TAU treatment as compared to the BtheB treatment. That’s a first sign that the BtheB intervention might outperform the treatment-as-usual!\nIn this chapter, we focus on a very simple version of a logistic regression: A model in which the dichotomous outcome variable is predicted by one categorical predictor variable, that is, the treatment condition (BtheB vs. TAU). Let’s assume that we plan to set up a study in which we want to scrutinize whether or not the “Beat the blues” intervention really outperforms the “treatment-as-usual” intervention with regard to the BDI scores two months after the end of the interventions. How many participants would we need for such a study to achieve 80% power?"
  },
  {
    "objectID": "GLM.html#estimating-the-population-parameters",
    "href": "GLM.html#estimating-the-population-parameters",
    "title": "Ch. 3: Generalized Linear Models",
    "section": "Estimating the population parameters",
    "text": "Estimating the population parameters\nAs in the previous chapters, we first need to estimate all population parameters relevant for this study. More specifically, we will need three estimates:\n\nthe proportion of participants in the BtheB condition\nthe probability of a favorable outcome in the BtheB condition\nthe probability of a favorable outcome in the TAU condition\n\nThe first parameter is pretty easy to estimate. In most cases, researchers will assign half of the participants to the treatment condition and the other half to a control condition. In this case, the probability of being in the BtheB condition would be 50%. Let’s store that in a variable called prop_btheb.\n\nprop_btheb <- 0.5\n\nThe other two estimates are only slightly more complicated to estimate. The probability of a favorable outcome in each of the conditions basically means: How many of the participants will not have a moderate/severe depression two months after completing the treatment-as-usual? And, how many of the participants will not have a moderate/severe depression two months after completing the Beat the blues intervention?\nThese two probabilities can only be estimated with solid pilot data. In our case, we can estimate both probabilities from the BtheB data set. The following chunk does that, rounds the estimates to two decimals, and stores the estimates in two objects called prop_tau and prop_btheb.\n\nprob_tau <- round(length(BtheB$treatment[BtheB$treatment == \"TAU\" & BtheB$bdi.2m.dicho == 0 & !is.na(BtheB$bdi.2m.dicho)]) / length(BtheB$treatment[BtheB$treatment == \"TAU\" & !is.na(BtheB$bdi.2m.dicho)]),2) \nprob_tau\n\n[1] 0.47\n\nprob_btheb <- round(length(BtheB$treatment[BtheB$treatment == \"BtheB\" & BtheB$bdi.2m.dicho == 0 & !is.na(BtheB$bdi.2m.dicho)]) / length(BtheB$treatment[BtheB$treatment == \"BtheB\" & !is.na(BtheB$bdi.2m.dicho)]),2)\nprob_btheb\n\n[1] 0.71\n\n\nIn the BtheB data set, the probability of not having a moderate/ severe depression in the BtheB condition was 0.71 and the same probability in the TAU condition was 0.47.\nNow we have all we need to start simulating data. There are multiple ways to simulate this kind of data, but one very simple way is to apply the sample() function. Here, we use it twice: Once to draw for the TAU condition and once for the BtheB condition (which differ in their probabilities of yielding a positive outcome, as we have seen before). For each condition, we draw whether or not the participant has a moderate/severe depression two months after the treatment, while coding “no moderate/severe depression” with 0 and “moderate/severe depression” with 1. Let’s try this by sampling 100 observations per condition.\n\nset.seed(8526)\n\n#sample from distribution in \"TAU\" condition\ntau <- cbind(rep(\"TAU\", 100), sample(x = c(0,1), replace = TRUE, prob = c(prob_tau, 1-prob_tau), size = 100))\n\n#sample from distribution in \"BtheB\" condition\nbtheb <- cbind(rep(\"BtheB\", 100), sample(x = c(0,1), replace = TRUE, prob = c(prob_btheb, 1-prob_btheb), size = 100))\n\nWe now have two data frames (labeled tau and btheB), one per condition. In the following chunk, we combine them into one data frame, change the variable names, transform the treatment variables to a factor, transform the outcome variable to an integer variable, and edit the factor levels – all of this is necessary to run our logistic regression on this data set later.\n\n#combine data frames\nsimulated_data <- rbind(tau, btheb) |> as.data.frame()\n\n#set variable names \ncolnames(simulated_data) <- c(\"treatment\", \"bdi.2m.dicho\")\n\n#change treatment variable to factor\nsimulated_data$treatment <- simulated_data$treatment |> as.factor()\n\n#change outcome variable to integer\nsimulated_data$bdi.2m.dicho <- simulated_data$bdi.2m.dicho |> as.integer()\n\n#reverse factor levels, this affects the coding of this factor in the regression analysis later\nsimulated_data$treatment <- factor(simulated_data$treatment, levels=rev(levels(simulated_data$treatment)))\n\nWith this first simulated data set, we can now perform a first logistic regression.\n\nsimulated_fit <- glm(bdi.2m.dicho ~ treatment, data = simulated_data, family = \"binomial\")\n\nsummary(simulated_fit)\n\n\nCall:\nglm(formula = bdi.2m.dicho ~ treatment, family = \"binomial\", \n    data = simulated_data)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.3354  -0.8615  -0.8615   1.0273   1.5305  \n\nCoefficients:\n               Estimate Std. Error z value Pr(>|z|)    \n(Intercept)      0.3640     0.2033   1.790   0.0734 .  \ntreatmentBtheB  -1.1641     0.2968  -3.922 8.78e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 275.26  on 199  degrees of freedom\nResidual deviance: 259.19  on 198  degrees of freedom\nAIC: 263.19\n\nNumber of Fisher Scoring iterations: 4\n\n\nHere, we find a negative and significant regression weight for the treatment predictor of -1.16, indicating that the BtheB treatment led to less moderate/severe depressions as compared to the TAU treatment. But, actually, it is not our central interest to test whether this particular simulated data set results in a significant treatment effect. What we really want to know is: How many of a theoretically infinite number of simulations yield a significant p-value of this effect? Thus, as in the previous chapters, we now repeatedly simulate data sets of a certain size from the specified population and store the results of the focal test (here: the p-value of the regression coefficient) in a vector called p_values."
  },
  {
    "objectID": "GLM.html#lets-do-the-power-analysis",
    "href": "GLM.html#lets-do-the-power-analysis",
    "title": "Ch. 3: Generalized Linear Models",
    "section": "Let’s do the power analysis",
    "text": "Let’s do the power analysis\n\n#write a function to automize the data simulation process\nsimulation <- function(n, p0, p1, prop = .50){\n  \n  #sample from distribution in \"TAU\" condition\n  tau <- cbind(rep(\"TAU\", n*prop), sample(x = c(0,1), replace = TRUE, prob = c(p0, 1-p0), size = n*prop))\n  \n  #sample from distribution in \"BtheB\" condition\n  btheb <- cbind(rep(\"BtheB\", n*prop), sample(x = c(0,1), replace = TRUE, prob = c(p1, 1-p1), size = n*prop))\n  \n  #combine both data sets and do some preprocessing\n  simulated_data <- rbind(tau, btheb) |> as.data.frame()\n  colnames(simulated_data) <- c(\"treatment\", \"bdi.2m.dicho\")\n  simulated_data$treatment <- simulated_data$treatment |> as.factor()\n  simulated_data$bdi.2m.dicho <- simulated_data$bdi.2m.dicho |> as.numeric()\n  simulated_data$treatment <-factor(simulated_data$treatment, levels=rev(levels(simulated_data$treatment)))\n  return(simulated_data)\n}\n\n\n#prepare empty vector to store the p-values\np_value <- NULL\n\n#prepare empty vector to store the results (i.e., the power per sample size)\nresults <- data.frame()\n\n\n# write function to store results of simulation\nsim <- function(n, p0, p1, prop = .50){\n  \n  #simulate data\n  data <- simulation(n = n, p0 = p0, p1 = p1, prop = .50)\n  \n  #run regression\n  simulated_fit <- glm(bdi.2m.dicho ~ treatment, data = data, family = \"binomial\")\n  \n  #store p-value\n  p_value <- coef(summary(simulated_fit))[2,4]\n  \n  #return p-value\n  return(p_value)\n\n}\n\n# set range of sample sizes\nns <- seq(from = 20, to = 500, by = 10)\n\n# set number of iterations\niterations <- 1000\n\n# perform power analysis\nfor(n in ns){\n  \np_values <- replicate(iterations, sim(n, p0 = prob_tau, p1 = prob_btheb, prop = prop))  \nresults <- rbind(results, data.frame(\n    n = n,\n    power = sum(p_values < .005)/iterations)\n  )\n}\n\nLet’s visualize the results.\n\nggplot(results, aes(x=n, y=power)) + geom_point() + geom_line() + scale_y_continuous(n.breaks = 10) + scale_x_continuous(n.breaks = 20) + geom_hline(yintercept= 0.8, color = \"red\")\n\n\n\n\nThis plot shows that we need approx. 220 participants to achieve 80% under the given assumptions. Note that this is the overall sample size, not the size per condition! That’s it – we have performed a simulation-based power analysis for a logistic regression!"
  },
  {
    "objectID": "GLM.html#verification-with-the-pwr2ppl-package",
    "href": "GLM.html#verification-with-the-pwr2ppl-package",
    "title": "Ch. 3: Generalized Linear Models",
    "section": "Verification with the pwr2ppl package",
    "text": "Verification with the pwr2ppl package\nLuckily, there are also R packages that can perform these kinds of power analyses, for example the pwr2ppl package (Aberson, 2019). We can use this package to verify our results. Does the pwr2ppl package yield the same result? Note that you will need to install this package if you haven’t used it before.\n\n#install.packages(\"devtools\")\n#devtools::install_github(\"chrisaberson/pwr2ppl\")\npwr2ppl::LRcat(p0 = prob_tau, p1 = prob_btheb, prop = .50,alpha = .005, power = .80)\n\nSample Size = 221 for Odds Ratio = 2.761\n\n\nThat’s basically the same result, well done!"
  },
  {
    "objectID": "GLM.html#using-a-safeguard-power-approach",
    "href": "GLM.html#using-a-safeguard-power-approach",
    "title": "Ch. 3: Generalized Linear Models",
    "section": "Using a safeguard power approach",
    "text": "Using a safeguard power approach\nOf note, our effect size estimates (i.e, the estimates of the probabilities of not having a moderate/severe depression two months after the BtheB or the TAU treatment) were so far based on a pilot study. However, this pilot study might not have yielded a precise estimate of these effect sizes. Thus, in order to consider uncertainty in these effect size estimates, it has been suggested to perform a safeguard power analysis (see chapter first chapter on linear models for more info) instead of a power analysis using the observed effect size. The main idea behind the safeguard power analysis is to compute a 60% confidence interval around the observed effect size, and to take the lower bound of this confidence interval as an effect size estimate (see Perugini et al., 2014). Let’s try this here.\nLet’s assume that we view the probability of not having a moderate/severe depression after the TAU treatment to be probably accurate, but that we want to account for uncertainty in the estimation of the probability of not having a moderate/severe depression after the BtheB treatment. We then simply calculate the 60% confidence interval around this effect size. We can use the BinomCI function from the DescTools package to do this. We need to provide this function with the number of successes (here: 37, see table above) and the number of observations (here: 52, see above).\n\nDescTools::BinomCI(37, 52, conf.level = 0.60)\n\n           est    lwr.ci   upr.ci\n[1,] 0.7115385 0.6560993 0.761292\n\n\nThis gives us an estimate of 0.66 for the lower bound of the 60% confidence interval of the probability of not having a moderate/severe depression after the BtheB treatment, while the point estimate for this effect size was 0.71. We can now redo our power analysis from above with this new effect size estimation. I am copying the chunk from above, while replacing the prob_btheb value with 0.66.\n\n#prepare empty vector to store the p-values\np_value <- NULL\n\n#prepare empty vector to store the results (i.e., the power per sample size)\nresults <- data.frame()\n\n# set range of sample sizes\nns <- seq(from = 20, to = 500, by = 10)\n\n# set number of iterations\niterations <- 1000\n\n# perform power analysis\nfor(n in ns){\n  \np_values <- replicate(iterations, sim(n, p0 = prob_tau, p1 = 0.66, prop = prop))  \nresults <- rbind(results, data.frame(\n    n = n,\n    power = sum(p_values < .005)/iterations)\n  )\n}\n\n#let's plot this\nggplot(results, aes(x=n, y=power)) + geom_point() + geom_line() + scale_y_continuous(n.breaks = 10) + scale_x_continuous(n.breaks = 20) + geom_hline(yintercept= 0.8, color = \"red\")\n\n\n\n\nHere, we get a total sample size of ca. 360 participants in order to ensure 80% power with our safeguard estimation."
  },
  {
    "objectID": "how_many_iterations.html",
    "href": "how_many_iterations.html",
    "title": "Bonus: How many Monte Carlo iterations are necessary?",
    "section": "",
    "text": "# Preparation: Install and load all necessary packages\n# install.packages(c(\"RcppArmadillo\", \"ggplot2\", \"patchwork\", \"pwr\"))\n\nlibrary(ggplot2)         # for plotting\nlibrary(RcppArmadillo)   # for fast LMs\nlibrary(patchwork)       # for arranging multiple ggplots\nlibrary(pwr)             # for analytical power analysis\n\nTo find the sample size needed for a study, we have previously use, say, 1000 iterations of data simulation and analysis, and varied the sample size n from, say, 100 to 1000, every 50, to find where the 80% power threshold was crossed (see LM1.qmd#sample-size-planning-find-the-necessary-sample-size). But is 1000 iterations giving a precise enough result?\nUsing the optimized code, we can explore how many Monte Carlo iterations are necessary to get stable computational results: we can re-run the same simulation with the same sample size and see how much random variation is between results!\n\n\n\n\n\n\nNote\n\n\n\nMonte Carlo methods, or Monte Carlo experiments, are a broad class of computational algorithms that rely on repeated random sampling to obtain numerical results. The underlying concept is to use randomness to solve problems that might be deterministic in principle. They are often used in physical and mathematical problems and are most useful when it is difficult or impossible to use other approaches. Monte Carlo methods are mainly used in three problem classes: optimization, numerical integration, and generating draws from a probability distribution.\n\n\nLet’s start with 1000 iterations (at n = 100, and 10 repetitions of the same power analysis):\n\nset.seed(0xBEEF)\niterations <- 1000\n\n# CHANGE: do the same sample size repeatedly, and see how much different runs deviate.\nns <- rep(100, 10)\n\nresult <- data.frame()\n\nfor (n in ns) {\n  \n  x <- cbind(\n    rep(1, n),\n    c(rep(0, n/2), rep(1, n/2))\n  )\n  \n  p_values <- rep(NA, iterations)\n  \n  for (i in 1:iterations) {\n    y <- 23 - 3*x[, 2] + rnorm(n, mean=0, sd=sqrt(117))\n    \n    mdl <- RcppArmadillo::fastLmPure(x, y)\n    pval <- 2*pt(abs(mdl$coefficients/mdl$stderr), mdl$df.residual, lower.tail=FALSE)\n    \n    p_values[i] <- pval[2]\n  }\n  \n  result <- rbind(result, data.frame(n = n, power = sum(p_values < .005)/iterations))\n}\n\nresult\n\n     n power\n1  100 0.065\n2  100 0.070\n3  100 0.068\n4  100 0.066\n5  100 0.067\n6  100 0.073\n7  100 0.071\n8  100 0.070\n9  100 0.071\n10 100 0.063\n\n\nAs you can see, the power estimates show some variance, ranging from 0.063 to 0.073. This can be formalized as the Monte Carlo error (MCE), which is define as “the standard deviation of the Monte Carlo estimator, taken across hypothetical repetitions of the simulation” (Koehler et al., 2009). With 1000 iterations (and 10 repetitions), this is:\n\nsd(result$power) |> round(4)\n\n[1] 0.0031\n\n\nWe only computed 10 repetitions of our power estimate, hence the MCE estimate is quite unstable. In the next computation, we will compute 100 repetitions of each power estimate (all with the same simulated sample size).\nHow much do we have to increase the iterations to achieve a MCE smaller than, say, 0.005 (i.e, an SD of +/- 0.5% of the power estimate)?\nLet’s loop through increasing iterations (this takes a few minutes):\n\niterations <- seq(1000, 6000, by=1000)\n\n# let's have 100 repetitions to get sufficiently stable MCE estimates\nns <- rep(100, 100)\nresult <- data.frame()\n\nfor (it in iterations) {\n\n  # print(it)  uncomment for showing the progress\n\n  for (n in ns) {\n    \n    x <- cbind(\n      rep(1, n),\n      c(rep(0, n/2), rep(1, n/2))\n    )\n    \n    p_values <- rep(NA, it)\n    \n    for (i in 1:it) {\n      y <- 23 - 3*x[, 2] + rnorm(n, mean=0, sd=sqrt(117))\n      \n      mdl <- RcppArmadillo::fastLmPure(x, y)\n      pval <- 2*pt(abs(mdl$coefficients/mdl$stderr), mdl$df.residual, lower.tail=FALSE)\n      \n      p_values[i] <- pval[2]\n    }\n    \n    result <- rbind(result, data.frame(iterations = it, n = n, power = sum(p_values < .005)/it))\n  }\n}\n\n# We can compute the exact power with the analytical solution:\nexact_power <- pwr.t.test(d = 3 / sqrt(117), sig.level = 0.005, n = 50)\n\np1 <- ggplot(result, aes(x=iterations, y=power)) + stat_summary(fun.data=mean_cl_normal) + ggtitle(\"Power estimate (error bars = SD)\") + geom_hline(yintercept = exact_power$power, colour = \"blue\", linetype = \"dashed\")\n\np2 <- ggplot(result, aes(x=iterations, y=power)) + stat_summary(fun=\"sd\", geom=\"point\") + ylab(\"MCE\") + ggtitle(\"Monte Carlo Error\")\n\np1/p2\n\n\n\n\nAs you can see, the MCE gets smaller with increasing iterations. The desired precision of MCE <= .005 can be achieved at around 3000 iterations (the dashed blue line is the exact power estimate from the analytical solution). While precision increases quickly by going from 1000 to 2000 iterations, further improvements are costly in terms of computation time. In sum, 3000 iterations seems to be a good compromise for this specific power simulation.\n\n\n\n\n\n\nNote\n\n\n\nThis choice of 3000 iterations does not necessarily generalize to other power simulations with other statistical models. But in my experience, 2000 iterations typically is a good (enough) choice. I often start with 500 iterations when exploring the parameter space (i.e., looking roughly for the range of reasonable sample sizes), and then “zoom” into this range with 2000 iterations.\n\n\nIn the lower plot, you can also see that the MCE estimate itself is a bit wiggly – we would expect a smooth curve. It suffers from meta-MCE! We could increase the precision of the MCE estimate by increasing the number of repetitions (currently at 100)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Simulations for Advanced Power Analyses",
    "section": "",
    "text": "This tutorial was created by Felix Schönbrodt and Moritz Fischer, with contributions from Malika Ihle, to be part of the training offering of the Ludwig-Maximilian University Open Science Center in Munich.\n\n\n\n\n\n\nNote\n\n\n\nThis book is still being developed. If you have comment to contribute to its improvement, you can submit pull requests in the respective .qmd file of the source repository by clicking on the ‘Edit this page’ and ‘Report an issue’ in the right navigation panel of each page."
  },
  {
    "objectID": "index.html#structure-of-the-tutorial",
    "href": "index.html#structure-of-the-tutorial",
    "title": "Simulations for Advanced Power Analyses",
    "section": "Structure of the tutorial",
    "text": "Structure of the tutorial\nDepending on your prior knowledge, you can fast forward some steps:\n\n\nAcquire necessary basic coding skills in R\nYou need to know R programming basics. If you are unfamiliar with R, you are advised to follow a self-paced basic tutorial prior to the workshop, e.g.: https://www.tutorialspoint.com/r up to “data reshaping” (this tutorial, for example, takes around 2h and covers all necessary basics).\nFor a higher-level introduction to R coding skills you can do the self-paced tutorial Introduction to simulation in R. This tutorial teaches how to simulate data and writing functions in R, with the goal to e.g.\n\ncheck alpha so your statistical models don’t yield more than 5% false-positive results\ncheck beta (power) for easy tests such as t-tests\nprepare a preregistration and make sure your code works\ncheck your understanding of statistics.\n\n\n\nComprehensive introduction to power analyses\nPlease read Chapter 1 of the SuperpowerBook by Aaron R. Caldwell, Daniël Lakens, Chelsea M. Parlett-Pelleriti, Guy Prochilo, and Frederik Aust.\nThis introduction covers sample effect sizes vs population effect sizes, how to take into account the uncertainty of the sample effect size to create a safeguard effect size to be used in power analyses, why post hoc power analyses are pointless, and why it is better to calculate the minimal detectable effect instead.\nThe rest of the Superpower book teaches how to use the superpower R package to simulate factorial designs and calculate power, which may be of great interest to you! In our tutorial, we chose to teach how to write simulation ‘by hand’ so you can understand the concept and adapt it to any of your designs.\n\n\nTutorial structure\nWith these prerequisites, you can start to learn power calculations for different complex models. Here are the type of models we will cover, you can pick and choose what is relevant to you:\n\nCh. 1: Linear Model I: a single dichotomous predictor\n\nCh. 2: Linear Model 2: Multiple predictors\n\nCh. 3: Generalized Linear Models\n\nCh. 4: Linear Mixed Models\n\nCh. 5: Structural Equation Modelling (SEM)\n\nWe recommend that everybody works through chapters 1 and 2, and then dive into the other chapters that are relevant.\nFor each model, we will follow the structure:\n\ndefine what type of data and variables need to be simulated, i.e. their distribution, their class (e.g. factor vs numerical value)\ngenerate data based on the equation of the model (data = model + error)\nrun the statistical test, and record the relevant statistic (e.g. p-value)\nreplicate step 2 and 3 to get the distribution of the statistic of interest (e.g. p-value)\nanalyze and interpret the combined results of many simulations i.e. check for which sample size you get at a significant result in 80% of the simulations\n\n\n\nInstall all packages\nThe following packages are necessary to reproduce the output of this tutorial. We recommend installing all of them before you dive into the individual chapters.\n\ninstall.packages(c(\n              \"ggplot2\", \n              \"ggdist\", \n              \"gghalves\", \n              \"pwr\", \n              \"MBESS\", \n              \"Rfast\", \n              \"DescTools\", \n              \"lme4\", \n              \"lmerTest\", \n              \"tidyr\", \n              \"Rfast\", \n              \"future.apply\", \n              \"lavaan\", \n              \"MASS\"), dependencies = TRUE, repos = \"https://cran.rstudio.com/\")\n\ninstall.packages(\"devtools\")\ndevtools::install_github(\"debruine/faux\")"
  },
  {
    "objectID": "index.html#license-funding-note",
    "href": "index.html#license-funding-note",
    "title": "Simulations for Advanced Power Analyses",
    "section": "License & Funding note",
    "text": "License & Funding note\nThis tutorial was initially commissioned and funded by the University of Hamburg, Faculty of Psychology and Movement Science.\nIt is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License."
  },
  {
    "objectID": "LM1.html",
    "href": "LM1.html",
    "title": "Ch. 1: Linear Model 1: A single dichotomous predictor",
    "section": "",
    "text": "Reading/working time: ~50 min.\nWe start with the simplest possible linear model: (a) a continuous outcome variable is predicted by a single dichotomous predictor. This model actually rephrases a t-test as a linear model! Then we build up increasingly complex models: (b) a single continuous predictor and (c) multiple continuous predictors (i.e., multiple regression)."
  },
  {
    "objectID": "LM1.html#get-some-real-data-as-starting-point",
    "href": "LM1.html#get-some-real-data-as-starting-point",
    "title": "Ch. 1: Linear Model 1: A single dichotomous predictor",
    "section": "Get some real data as starting point",
    "text": "Get some real data as starting point\n\n\n\n\n\n\nNote\n\n\n\nThe creators of this tutorial are no experts in clinical psychology; we opportunistically selected open data sets based on their availability. Usually, we would look for meta-analyses – ideally bias-corrected – for more comprehensive evidence.\n\n\nThe R package HSAUR contains open data on 100 depressive patients, where 50 received treatment-as-usual (TAU) and 50 received a new treatment (“Beat the blues”; BtheB). Data was collected in a pre-post-design with several follow-up measurements. For the moment, we focus on the pre-treatment baseline value (bdi.pre) and the first post-treatment value (bdi.2m). We will use that data set as a “pilot study” for our power analysis.\nNote that this pilot data does not contain an inactive control group, such as the waiting list group that we assume for our planned study. Both the BtheB and the TAU group are active treatment groups. Nonetheless, we will be able to infer our treatment effect (vs. an inactive control group) from that data by looking at the pre-treatment data.\n\n# the data can be found in the HSAUR package, must be installed first\n#install.packages(\"HSAUR\")\n\n# load the data\ndata(\"BtheB\", package = \"HSAUR\")\n\n# get some information about the data set:\n?HSAUR::BtheB\n\nhist(BtheB$bdi.pre)\n\n\n\n\nThe standardized cutoffs for the BDI are:\n\n0–13: minimal depression\n14–19: mild depression\n20–28: moderate depression\n29–63: severe depression.\n\nReturning to our questions from above:\nWhat BDI values would we expect on average in our sample before treatment?\n\n# we take the pre-score here:\nmean(BtheB$bdi.pre)\n\n[1] 23.33\n\n\nThe average BDI score before treatment was 23, corresponding to a “moderate depression”.\n\nWhat variability would we expect in our sample?\n\n\nvar(BtheB$bdi.pre)\n\n[1] 117.5163\n\n\n\nWhat average treatment effect would we expect?\n\n\n# we take the 2 month follow-up measurement, \n# separately for the  \"treatment as usual\" and \n# the \"Beat the blues\" group:\nmean(BtheB$bdi.2m[BtheB$treatment == \"TAU\"], na.rm=TRUE)\n\n[1] 19.46667\n\nmean(BtheB$bdi.2m[BtheB$treatment == \"BtheB\"])\n\n[1] 14.71154\n\n\nHence, the two treatments reduced BDI scores from an average of 23 to 19 (TAU) and 15 (BtheB). Based on that data set, we can conclude that a typical treatment effect is somewhere between a 4 and a 8-point reduction of BDI scores.1\nFor our purpose, we compute the average treatment effect combined for both treatments. The average post-treatment score is:\n\nmean(BtheB$bdi.2m, na.rm=TRUE)\n\n[1] 16.91753\n\n\nSo, the average reduction across both treatments is 23-17=6. In the following scripts, we’ll use that value as our assumed treatment effect."
  },
  {
    "objectID": "LM1.html#enter-specific-values-for-the-model-parameters",
    "href": "LM1.html#enter-specific-values-for-the-model-parameters",
    "title": "Ch. 1: Linear Model 1: A single dichotomous predictor",
    "section": "Enter specific values for the model parameters",
    "text": "Enter specific values for the model parameters\nLet’s rewrite the abstract equation with the specific variable names. We first write the equation for the systematic part (without the error term). This also represents the predicted value:\n\\widehat{\\text{BDI}} = b_0 + b_1*\\text{treatment}\nWe use the notation \\widehat{\\text{BDI}} (with a hat) to denote the predicted BDI score.\nThe predicted score for the control group then simply is the intercept of the model, as the second term is erased by entering the value “0” for the control group:\n\\widehat{\\text{BDI}} = b_0 + b_1*0 = b_0\nThe predicted score for the treatment group is the value for the control group plus the regression weight:\n\\widehat{\\text{BDI}} = b_0 + b_1*1 Hence, the regression weight (aka. “slope parameter”) b_1 estimates the mean difference between both groups, which is the treatment effect.\nWith our knowledge from the open BDI data, we insert plausible values for the intercept b_0 and the treatment effect b_1. We expect a reduction of the depression score, so the treatment effect is assumed to be negative. We take the combined treatment effect of the two pilot treatments. And as power analysis is not rocket science, we generously round the values:\n\\widehat{\\text{BDI}} = 23 - 6*treatment\nHence, the predicted value is 23 - 6*0 = 23 for the control group, and 23 - 6*1 = 17 for the treatment group.\nWith the current model, all participants in the control group have the same predicted value (23), as do all participants in the treatment group (17).\nAs a final step, we add the random noise to the model, based on the variance in the pilot data:\n\\text{BDI} = 23 - 6*treatment + e; e \\sim N(0, var=117) \nThat’s our final equation with assumed population parameters! With that equation, we assume a certain state of reality and can sample “virtual participants”."
  },
  {
    "objectID": "LM1.html#what-is-the-effect-size-in-the-model",
    "href": "LM1.html#what-is-the-effect-size-in-the-model",
    "title": "Ch. 1: Linear Model 1: A single dichotomous predictor",
    "section": "What is the effect size in the model?",
    "text": "What is the effect size in the model?\nResearchers often have been trained to think in standardized effect sizes, such as Cohen’s d, a correlation r, or other indices such as f^2 or partial \\eta^2. In the simulation approach, we typical work on the raw scale of variables.\nThe raw effect size is simply the treatment effect on the original BDI scale (i.e., the group difference in the outcome variable). In our case we assume that the treatment lowers the BDI score by 6 points, on average. Defining the raw effect requires some domain knowledge – you need to know your measurement scale, and you need to know what the values (and differences between values) mean. In our example, a reduction of 6 BDI points means that the average patient moves from a moderate depression (23 points) to a mild depression (17 points). Working with raw effect sizes forces you to think about your actual data (instead of plugging in content-free default standardized effect sizes), and enables you to do plausibility checks on your simulation.\nThe standardized effect size relates the raw effect size to the unexplained error variance. In the two-group example, this can be expressed as Cohen’s d, which is the mean difference divided by the standard deviation (SD):\nd = \\frac{M_{treat} - M_{control}}{SD} = \\frac{17 - 23}{\\sqrt{117}} = -0.55\nThe standardized effect size always relates two components: The raw effect size (here: 6 points difference) and the error variance. Hence, you can increase the standardized effect size by (a) increasing the raw treatment effect, or (b) reducing the error variance.\n\n\n\n\n\n\nNote\n\n\n\nIf you look up the formula of Cohen’s d, it typically uses the pooled SD from both groups. As we assumed that both groups have the same SD, we simply took that value."
  },
  {
    "objectID": "LM1.html#doing-the-power-analysis",
    "href": "LM1.html#doing-the-power-analysis",
    "title": "Ch. 1: Linear Model 1: A single dichotomous predictor",
    "section": "Doing the power analysis",
    "text": "Doing the power analysis\nNow we need to repeatedly draw many samples and see how many of the analyses would have detected the existing effect. To do this, we put the code from above into a function called sim. We coded the function to either return the focal p-value (as default) or to print a model summary (helpful for debugging and testing the function). This function takes two parameters:\n\nn defines the required sample size\ntreatment_effect defines the treatment effect in the raw scale (i.e., reduction in BDI points)\n\nWe then use the replicate function to repeatedly call the sim function for 1000 iterations.\n\nset.seed(0xBEEF)\n\niterations <- 1000 # the number of Monte Carlo repetitions\nn <- 100 # the size of our simulated sample\n\nsim1 <- function(n=100, treatment_effect=-6, print=FALSE) {\n  treatment <- c(rep(0, n/2), rep(1, n/2))\n  BDI <- 23 + treatment_effect*treatment + rnorm(n, mean=0, sd=sqrt(117))\n  \n  # this lm() call should be exactly the function that you use\n  # to analyse your real data set\n  res <- lm(BDI ~ treatment)\n  p_value <- summary(res)$coefficients[\"treatment\", \"Pr(>|t|)\"]\n  \n  if (print==TRUE) print(summary(res))\n  else return(p_value)\n}\n\n# now run the sim() function a 1000 times and store the p-values in a vector:\np_values <- replicate(iterations, sim1(n=100))\n\nHow many of our 1000 virtual samples would have found the effect?\n\ntable(p_values < .005)\n\n\nFALSE  TRUE \n  535   465 \n\n\nOnly 46% of samples with the same size of n=100 result in a significant p-value.\n46% – that is our power for \\alpha = .005, Cohen’s d=.55, and n=100."
  },
  {
    "objectID": "LM1.html#sample-size-planning-find-the-necessary-sample-size",
    "href": "LM1.html#sample-size-planning-find-the-necessary-sample-size",
    "title": "Ch. 1: Linear Model 1: A single dichotomous predictor",
    "section": "Sample size planning: Find the necessary sample size",
    "text": "Sample size planning: Find the necessary sample size\nNow we know that a sample size of 100 does not lead to a sufficient power. But what sample size would we need to achieve a power of at least 80%? In the simulation approach you need to test different ns until you find the necessary sample size. We do this by wrapping the simulation code into a loop that continuously increases the n. We then store the computed power for each n.\n\nset.seed(0xBEEF)\n\n# define all predictor and simulation variables.\niterations <- 1000\nns <- seq(100, 300, by=20) # test ns between 100 and 300\n\nresult <- data.frame()\n\nfor (n in ns) {  # loop through elements of the vector \"ns\"\n  p_values <- replicate(iterations, sim1(n=n))\n  \n  result <- rbind(result, data.frame(\n    n = n,\n    power = sum(p_values < .005)/iterations)\n  )\n  \n  # show the result after each run (not shown here in the tutorial)\n  print(result)\n}\n\nLet’s plot there result:\n\nggplot(result, aes(x=n, y=power)) + geom_point() + geom_line()\n\n\n\n\nHence, with n=180 (90 in each group), we have a 80% chance to detect the effect.\n🥳 Congratulations! You did your first power analysis by simulation. 🎉\nFor these simple models, we can also compute analytic solutions. Let’s verify our results with the pwr package – a linear regression with a single dichotomous predictor is equivalent to a t-test:\n\npwr.t.test(d = 0.55, sig.level = 0.005, power = .80)\n\n\n     Two-sample t test power calculation \n\n              n = 90.00212\n              d = 0.55\n      sig.level = 0.005\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nExactly the same result – phew 😅"
  },
  {
    "objectID": "LM1.html#safeguard-power-analysis",
    "href": "LM1.html#safeguard-power-analysis",
    "title": "Ch. 1: Linear Model 1: A single dichotomous predictor",
    "section": "Safeguard power analysis",
    "text": "Safeguard power analysis\nAs sensitivity analysis, we will apply a safeguard power analysis (Perugini et al., 2014) that aims for the lower end of a two-sided 60% CI around the parameter of the treatment effect (the intercept is irrelevant). (Of course you can use any other value than 60%, but this is the value (tentatively) mentioned by the inventors of the safeguard power analysis.)\n\n\n\n\n\n\nNote\n\n\n\nIf you assume publication bias, another heuristic for aiming at a more realistic population effect size is the “divide-by-2” heuristic. (see kickoff presentation)\n\n\nWe can use the ci.smd function from the MBESS package to compute a CI around Cohen’s d that we computed for our treatment effect:\n\nci.smd(smd=-0.55, n.1=50, n.2=50, conf.level=.60)\n\n$Lower.Conf.Limit.smd\n[1] -0.7201263\n\n$smd\n[1] -0.55\n\n$Upper.Conf.Limit.smd\n[1] -0.377061\n\n\nHowever, in the simulated regression equation, we need the raw effect size – so we have to backtransform the standardized confidence limits into the original metric. As the assumed effect is negative, we aim for the upper, i.e., the more conservative limit. After backtransformation in the raw metric, it is considerably smaller, at -4.1:\nd = \\frac{M_{diff}}{SD} \\Rightarrow M_{diff} = d*SD = -0.377 * \\sqrt{117} = -4.08\nNow we can rerun the power simulation with this more conservative value (the only change to the code above is that we changed the treatment effect from -6 to -4.1).\n\n\nShow the code\nset.seed(0xBEEF)\n\n# define all predictor and simulation variables.\niterations <- 1000\nns <- seq(200, 400, by=20) # test ns between 200 and 400\n\nresult <- data.frame()\n\nfor (n in ns) {  # loop through elements of the vector \"ns\"\n  p_values <- replicate(iterations, sim1(n=n, treatment_effect = -4.1))\n  \n  result <- rbind(result, data.frame(\n    n = n,\n    power = sum(p_values < .005)/iterations)\n  )\n  \n  # show the result after each run\n  print(result)\n}\n\n\n\n\n     n power\n1  200 0.448\n2  220 0.466\n3  240 0.549\n4  260 0.584\n5  280 0.646\n6  300 0.664\n7  320 0.708\n8  340 0.744\n9  360 0.790\n10 380 0.813\n11 400 0.822\n\n\nWith that more conservative effect size assumption, we would need around 380 participants, i.e. 190 per group."
  },
  {
    "objectID": "LM1.html#smallest-effect-size-of-interest-sesoi",
    "href": "LM1.html#smallest-effect-size-of-interest-sesoi",
    "title": "Ch. 1: Linear Model 1: A single dichotomous predictor",
    "section": "Smallest effect size of interest (SESOI)",
    "text": "Smallest effect size of interest (SESOI)\nMany methodologists argue that we should not power for the expected effect size, but rather for the smallest effect size of interest (SESOI). In this case, a non-significant result can be interpreted as “We accept the H_0, and even if a real effect existed, it most likely is too small to be relevant”.\nWhat change of BDI scores is perceived as “clinically important”? The hard part is to find a convincing theoretical or empirical argument for the chosen SESOI. In the case of the BDI, luckily someone else did that work.\nThe NICE guidance suggest that a change of >=3 BDI-II points is clinically important.\nHowever, as you can expect, things are more complicated. Button et al. (2015) analyzed data sets where patients have been asked, after a treatment, whether they felt “better”, “the same” or “worse”. With these subjective ratings, they could relate changes in BDI-II scores to perceived improvements. Hence, even when depressive symptoms were measurably reduced in the BDI, patients still might answer “feels the same”, which indicates that the reduction did not surpass a threshold of subjective relevant improvement. But the minimal clinical importance depends on the baseline severity: For patients to feel notably better, they need more reduction of BDI-II scores if they start from a higher level of depressive symptoms. Following from this analysis, typical SESOIs are higher than the NICE guidelines, more in the range of -6 BDI points.\nFor our example, let’s use the NICE recommendation of -3 BDI points as a lower threshold for our power analysis (anything larger than that will be covered anyway).\n\n\nShow the code\nset.seed(0xBEEF)\n\n# define all predictor and simulation variables.\niterations <- 1000\n\n# CHANGE: we adjusted the range of probed sample sizes upwards, as the effect size now is considerably smaller\nns <- seq(600, 800, by=20)\n\nresult <- data.frame()\n\nfor (n in ns) {\n  p_values <- replicate(iterations, sim1(n=n, treatment_effect = -3))\n  \n  result <- rbind(result, data.frame(\n    n = n,\n    power = sum(p_values < .005)/iterations)\n  )\n\n  print(result)\n}\n\n\n\n\n     n power\n1  600 0.728\n2  620 0.738\n3  640 0.756\n4  660 0.781\n5  680 0.791\n6  700 0.791\n7  720 0.827\n8  740 0.820\n9  760 0.852\n10 780 0.866\n11 800 0.871\n\n\nHence, we need around 700 participants to reliably detect this smallest effect size of interest.\nDid you spot the strange pattern in the result? At n=720, the power is 83%, but only 82% with n=740? This is not possible, as power monotonically increases with sample size. It suggests that this is simply Monte Carlo sampling error – 1000 iterations are not enough to get precise estimates. When we increase iterations to 10,000, it takes much longer, but gives more precise results:\n\n\nShow the code\nset.seed(0xBEEF)\n\n# define all predictor and simulation variables.\niterations <- 10000\nns <- seq(640, 740, by=20)\n\nresult <- data.frame()\n\nfor (n in ns) {\n  p_values <- replicate(iterations, sim1(n=n, treatment_effect = -3))\n  \n  result <- rbind(result, data.frame(\n    n = n,\n    power = sum(p_values < .005)/iterations)\n  )\n\n  print(result)\n}\n\n\n\n\n    n  power\n1 640 0.7583\n2 660 0.7758\n3 680 0.7865\n4 700 0.8060\n5 720 0.8192\n6 740 0.8273\n\n\nNow power increases monotonically with sample size, as expected.\nTo explore how many Monte Carlo iterations are necessary to get stable computational results, see the bonus page Bonus: How many Monte Carlo iterations are necessary?"
  },
  {
    "objectID": "LM2.html",
    "href": "LM2.html",
    "title": "Ch. 2: Linear Model 2: Multiple predictors",
    "section": "",
    "text": "Reading/working time: ~50 min.\nIn the first chapter on linear models, we had the simplest possible linear model: a continuous outcome variable is predicted by a single dichotomous predictor. In this chapter, we build up increasingly complex models by (a) adding a single continuous predictor and (b) modeling an interaction."
  },
  {
    "objectID": "LM2.html#get-some-real-data-as-starting-point",
    "href": "LM2.html#get-some-real-data-as-starting-point",
    "title": "Ch. 2: Linear Model 2: Multiple predictors",
    "section": "Get some real data as starting point",
    "text": "Get some real data as starting point\nInstead of guessing the necessary quantities – in the current case, the pre-post-correlation – let’s look at real data. The “Beat the blues” (BtheB) data set from the HSAUR R package contains pre-treatment baseline values (bdi.pre), along with multiple post-treatment values. Here we focus on the first post-treatment assessment, 2 months after the treatment (bdi.2m).\n\n# load the data\ndata(\"BtheB\", package = \"HSAUR\")\n\n# pre-post-correlation\ncor(BtheB$bdi.pre, BtheB$bdi.2m, use=\"p\")\n\n[1] 0.6142207\n\n\nIn our pilot data set, the pre-post-correlation is around r=.6. We will use this value in our simulations."
  },
  {
    "objectID": "LM2.html#update-the-sim-function",
    "href": "LM2.html#update-the-sim-function",
    "title": "Ch. 2: Linear Model 2: Multiple predictors",
    "section": "Update the sim() function",
    "text": "Update the sim() function\nNow we add the continuous predictor into our sim() function. In order to simulate a correlated variable, we need to slightly change the workflow in our simulation.\nWe use the rmvnorm function from the Rfast package to create correlated, normally distributed variables. It takes three parameters:\n\nn: The number of random observations\nmu: A vector of mean values (one for each random variable)\nsigma: The variance-covariance-matrix of the random variables.\n\nFor mu, we use the value 23 for both the pre and the post value. You can imagine that we only look at the control group: The mean value is not supposed to change systematically from pre to post (although each individual person can go somewhat up or down).\nThe variance-covariance-matrix defines two properties at once: The variance of each variable, and the covariance to all other variables. In the two-variable case, the general matrix looks like:\n\n\\begin{bmatrix}\nvar_x & cov_{xy}\\\\\ncov_{xy} & var_y\n\\end{bmatrix}\n\nNote that the covariance in the diagonal has the same values, as cov_{xy} = cov_{yx}. As we need to enter the covariance into sigma, we need to convert the correlation into a covariance. Here’s the formula:\ncor_{xy} = \\frac{cov_{xy}}{\\sigma_x\\sigma_y}\n(Note: The denominator contains the standard deviation, not the variance.) Solved for the covariance yields:\ncov_{xy} = cor_{xy}\\sigma_X\\sigma_y = 0.6 * \\sqrt{117} * \\sqrt{117} = 70.2\nHence, the specific variance-covariance-matrix sigma is in our case (generously rounded):\n\n\\begin{bmatrix}\n117 & 70\\\\\n70 & 117\n\\end{bmatrix}\n\nPut it all together:\n\nset.seed(0xBEEF)\nmu <- c(23, 23) # the mean values of both variables\nsigma <- matrix(\n    c(117 , 70, \n       70, 117), nrow=2, byrow=TRUE)\ndf <- rmvnorm(n=10000, mu=mu, sigma=sigma) |> data.frame()\nnames(df) <- c(\"BDI_pre\", \"BDI_post0\")\n\n# Check: in a large sample the correlation should be close to .6\ncor(df)\n\n            BDI_pre BDI_post0\nBDI_pre   1.0000000 0.5978088\nBDI_post0 0.5978088 1.0000000\n\n\n\n\n\n\n\n\nNote\n\n\n\nA very nice and user-friendly alternative for simulating correlated variables is the rnorm_multi function from the faux package.\n\n\nWe now have the correlated BDI_{pre} and BDI_{post} scores. Finally, we have to impose the treatment effect onto the post variable: In the control group, the mean value stays constant at 23 (what we already have simulated), in the treatment group, the BDI is 6 points lower:\n\n# add treatment predictor variables\ndf$treatment <- rep(c(0, 1), times=nrow(df)/2)\n  \n# add the treatment effect to the BDI_post0 variable\ndf$BDI_post <- df$BDI_post0 + -6*df$treatment\n\nWe do not need to add an explicit intercept, as this is already encoded in the mean value of the simulated variables. (Alternatively, we could have simulated them centered on zero and then explicitly add the intercept in the model equation).\nLet’s make a plausibility check by plotting the simulated variables. We first have to convert them into long format for a nicer plot:\n\n# reduce to 100 cases for plotting; define factors\ndf2 <- df[1:400, ]\ndf2$id <- 1:nrow(df2)\ndf2$BDI_post0 <- NULL\n\ndf_long <- pivot_longer(df2, cols=c(BDI_pre, BDI_post), names_to=\"time\")\ndf_long$time <- factor(df_long$time, levels=c(\"BDI_pre\", \"BDI_post\"))\ndf_long$treatment <- factor(df_long$treatment, levels=c(0, 1), labels=c(\"Control\", \"Treatment\"))\n\nggplot(df_long, aes(x=time, y=value, group=id)) + geom_point() + geom_line() + facet_wrap(~treatment)\n\n\n\nggplot(df_long, aes(x=time, y=value, group=time)) + geom_boxplot() + facet_wrap(~treatment)\n\n\n\n\nLooks good – reasonable BDI values, same means in control group, same variance. The treatment effect of -6 is visible.\nLet’s put that together in the new sim function:\n\nsim2 <- function(n=100, treatment_effect=-6, pre_post_cor = 0.6, err_var = 117, print=FALSE) {\n  library(Rfast)\n  \n  # Here we simulate correlated BDI pre and post scores\n  mu <- c(23, 23) # the mean values of both variables\n  sigma <- matrix(\n    c(err_var, pre_post_cor*sqrt(err_var)*sqrt(err_var), \n      pre_post_cor*sqrt(err_var)*sqrt(err_var), err_var), \n    nrow=2, byrow=TRUE)\n  df <- rmvnorm(n, mu, sigma) |> data.frame()\n  names(df) <- c(\"BDI_pre\", \"BDI_post0\")\n  \n  # add treatment predictor variables\n  df$treatment <- c(rep(0, n/2), rep(1, n/2))\n  \n  # center the BDI_pre value for better interpretability\n  df$BDI_pre.c <- df$BDI_pre - mean(df$BDI_pre)\n  \n  # add the treatment effect to the BDI_post0 variable\n  # We do not need to add an intercept, as this is already encoded\n  # in the mean value of the simulated variables.\n  df$BDI_post <- df$BDI_post0 + df$treatment*treatment_effect\n  \n  # fit the model\n  res <- lm(BDI_post ~ BDI_pre.c + treatment, data=df)\n  summary(res)\n  p_value <- summary(res)$coefficients[\"treatment\", \"Pr(>|t|)\"]\n  \n  if (print==TRUE) print(summary(res))\n  else return(p_value) \n}\n\nLet’s test the plausibility of the new sim2() function by simulating a very large sample – this should give estimates close to the true values. We also vary the assumed pre-post-correlation. When this is 0, the results should be identical to the simpler model from Chapter 1 (except one df that we lost due to the additional predictor). When the pre-post-correlation is > 0, the error variance should be reduced (see Residual standard error at the bottom of each lm output).\n\nset.seed(0xBEEF)\n\n# without pre_post_cor, the result should be the same\nsim2(n=100000, pre_post_cor=0, print=TRUE)\n\n\nCall:\nlm(formula = BDI_post ~ BDI_pre.c + treatment, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-50.580  -7.401   0.021   7.381  47.636 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 23.039486   0.048743 472.676   <2e-16 ***\nBDI_pre.c   -0.002105   0.003178  -0.662    0.508    \ntreatment   -6.091742   0.068933 -88.372   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.9 on 99997 degrees of freedom\nMultiple R-squared:  0.07244,   Adjusted R-squared:  0.07242 \nF-statistic:  3905 on 2 and 99997 DF,  p-value: < 2.2e-16\n\n# with pre_post_cor, the residual error should be reduced\nsim2(n=100000, pre_post_cor=0.6, print=TRUE)\n\n\nCall:\nlm(formula = BDI_post ~ BDI_pre.c + treatment, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-35.915  -5.846   0.005   5.807  35.457 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 22.942640   0.038783   591.6   <2e-16 ***\nBDI_pre.c    0.597809   0.002538   235.5   <2e-16 ***\ntreatment   -5.956531   0.054847  -108.6   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.672 on 99997 degrees of freedom\nMultiple R-squared:  0.4017,    Adjusted R-squared:  0.4017 \nF-statistic: 3.357e+04 on 2 and 99997 DF,  p-value: < 2.2e-16\n\n\nIndeed, with pre_post_cor = 0 the parameter estimates are identical, and with non-zero pre-post-correlation the error term gets reduced.\n\n\n\n\n\n\nAn additional plausibility check (advanced)\n\n\n\n\n\nWe assume independence of both predictor variables (BDI_pre and treatment). This is plausible, because the treatment was randomized and therefore independent from the baseline. In this case, the variance that each predictor explains in the dependent variable is additive. The pre-measurement explains r^2 = .6^2 = 36\\% of the variance in the post-measurement. As this variance is unrelated to the treatment factor, it reduces the variance of the error term (which was at 117) by 36%:\n\\sigma^2_{err} = 117 * (1-0.36) = 74.88\nThe square root of this unexplained error variance is the Residual standard error from the lm output: \\sqrt{74.88} = 8.65."
  },
  {
    "objectID": "LM2.html#do-the-power-analysis",
    "href": "LM2.html#do-the-power-analysis",
    "title": "Ch. 2: Linear Model 2: Multiple predictors",
    "section": "Do the power analysis",
    "text": "Do the power analysis\nThe next step is the same as always: use the replicate function to repeatedly call the sim function for many iterations, and increase the simulated sample size until the desired power level is achieved.\n\n\nCode\nset.seed(0xBEEF)\n\n# define all predictor and simulation variables.\niterations <- 2000\nns <- seq(90, 180, by=10) # ns are already adjusted to cover the relevant range\n\nresult <- data.frame()\n\nfor (n in ns) {  # loop through elements of the vector \"ns\"\n  p_values <- replicate(iterations, sim2(n=n, pre_post_cor = 0.6))\n  \n  result <- rbind(result, data.frame(\n      n = n,\n      power = sum(p_values < .005)/iterations\n    )\n  )\n  \n  # show the result after each run (not shown here in the tutorial)\n  print(result)\n}\n\n\n\n\n     n  power\n1   90 0.6690\n2  100 0.7245\n3  110 0.7705\n4  120 0.8185\n5  130 0.8765\n6  140 0.8890\n7  150 0.9335\n8  160 0.9335\n9  170 0.9465\n10 180 0.9675\n\n\nIn the original analysis, we needed n=180 (90 in each group) for 80% power. Including the baseline covariate (which explains r^2 = .6^2 = 36\\% of the variance in post scores) reduces that number to around n=115.\nLet’s check the plausibility of our power simulation. Borm et al. (2007, p. 1237) propose a simple method on how to arrive at a planned sample size when switching from a simple t-test (comparing post-treatment groups) to a model that controls for the baseline:\n\n“We propose a simple method for the sample size calculation when ANCOVA is used: multiply the number of subjects required for the t-test by (1-r^2) and add one extra subject per group.\n\nWhen we enter our assumed pre-post-correlation into that formula, we arrive a n=117 – very close to our value:\n180 * (1 - .6^2) + 2 = 117"
  },
  {
    "objectID": "LM2.html#thinking-visually",
    "href": "LM2.html#thinking-visually",
    "title": "Ch. 2: Linear Model 2: Multiple predictors",
    "section": "Thinking visually",
    "text": "Thinking visually\nArriving at plausible guesses for interaction effects can be tricky. We strongly recommend to visualize your assumed interaction effect in a plot – first drawn by hand: Do you expect a disordinal (i.e., cross-over) interaction, or an ordinal one where the effect is amplified (but not reversed) by the moderating variable?\nLook at a reasonable maximum and minimum of your variables – how large would you expect the effect to be there?\nEvaluate the effect size in subgroups: For example, imagine a group of really severely depressed patients. And then assume that the therapy works exceptionally well for them – what would be a realistic outcome for them? Would you expect them all to be at BDI<10? Probably not. Or would a very good outcome simply be that they move from a “severe depression” to a “moderate depression”? This gives you an estimate of the upper limit of your effect size. After defining upper limits, you should ask: What effect size would be plausible, given your background knowledge of typical effects in your field?\nOnly when you have drawn a reasonable plot by hand (and validated that with colleagues), start to work out the parameter values that you need to enter in the regression equations in order to arrive at the desired interaction plot."
  },
  {
    "objectID": "LM2.html#do-the-power-analysis-1",
    "href": "LM2.html#do-the-power-analysis-1",
    "title": "Ch. 2: Linear Model 2: Multiple predictors",
    "section": "Do the power analysis",
    "text": "Do the power analysis\nNow that we have our desired parameter values, we update our sim() function by:\n\nImposing the interaction effect onto our dependent variable\nAdding the interaction effect to our lm analysis model\nExtracting two p-values: We now want to compute the power both for the treatment main effect and for the interaction term.\n\n\n# CHANGE: Add interaction_effect\nsim3 <- function(n=100, treatment_effect=-6, pre_post_cor = 0.6, \n                 interaction_effect = -.2, err_var = 117, print=FALSE) {\n  library(Rfast)\n  mu <- c(23, 23) # the mean values of both variables\n  sigma <- matrix(\n    c(err_var, pre_post_cor*sqrt(err_var)*sqrt(err_var), \n      pre_post_cor*sqrt(err_var)*sqrt(err_var), err_var), \n    nrow=2, byrow=TRUE)\n  df <- rmvnorm(n, mu, sigma) |> data.frame()\n  names(df) <- c(\"BDI_pre\", \"BDI_post0\")\n  \n  # add treatment predictor variables\n  df$treatment <- c(rep(0, n/2), rep(1, n/2))\n  \n  # center the BDI_pre value for better interpretability\n  df$BDI_pre.c <- df$BDI_pre - mean(df$BDI_pre)\n  \n  # CHANGE: add the treatment main effect and the interaction to the BDI_post variable\n  df$BDI_post <- df$BDI_post0 + treatment_effect*df$treatment + interaction_effect*df$treatment*df$BDI_pre.c\n  \n  # CHANGE: Add interaction effect to analysis model\n  res <- lm(BDI_post ~ BDI_pre.c * treatment, data=df)\n  \n  # CHANGE: extract both focal p-values\n  p_values <- summary(res)$coefficients[c(\"treatment\", \"BDI_pre.c:treatment\"), \"Pr(>|t|)\"]\n  \n  if (print==TRUE) print(summary(res))\n  else return(p_values) \n}\n\n\nset.seed(0xBEEF)\n\n# define all predictor and simulation variables.\niterations <- 1000\nns <- seq(100, 400, by=20)\n\nresult <- data.frame()\n\nfor (n in ns) {  # loop through elements of the vector \"ns\"\n  p_values <- replicate(iterations, sim3(n=n, pre_post_cor = 0.6, interaction_effect = -.2))\n  \n  # CHANGE: Analyze both sets of p-values separately\n  # The p-values for the main effect are stored in the first row,\n  # the p-values for the interaction effect are stored in the 2nd row\n  \n  result <- rbind(result, data.frame(\n      n = n,\n      power_treatment = sum(p_values[1, ] < .005)/iterations,\n      power_interaction = sum(p_values[2, ] < .005)/iterations\n    )\n  )\n  \n  # show the result after each run (not shown here in the tutorial)\n  print(result)\n}\n\n\n\n     n power_treatment power_interaction\n1  100           0.709             0.058\n2  120           0.830             0.069\n3  140           0.900             0.084\n4  160           0.932             0.118\n5  180           0.962             0.122\n6  200           0.991             0.129\n7  220           0.987             0.158\n8  240           0.997             0.210\n9  260           0.996             0.189\n10 280           0.999             0.212\n11 300           0.999             0.262\n12 320           1.000             0.257\n13 340           0.998             0.293\n14 360           1.000             0.338\n15 380           1.000             0.385\n16 400           1.000             0.370\n\n\nWhile the power for detecting the main effect quickly approaches 100%, even 400 participants are by far not enough to detect the interaction effect reliably. (In particular when you recall that we set the assumed interaction effect to the largest plausible value)."
  },
  {
    "objectID": "LMM.html",
    "href": "LMM.html",
    "title": "Ch. 4: Linear Mixed Models / Multilevel models",
    "section": "",
    "text": "Reading/working time: ~60 min.\nIn order to keep this chapter at a reasonable length, we focus on how to simulate the mixed effects/multi-level model and spend less time discussing how to arrive at plausible parameter values. Please read Chapters 1 and 2 for multiple examples where the parameter values are derived from (a) the literature (e.g., bias-corrected meta-analyses), (b) pilot data, or (c) plausibility constraints.\nFurthermore, we’ll use the techniques described in the Chapter “Bonus: Optimizing R code for speed”, otherwise the simulations are too slow. If you wonder about the unknown commands in the code, please read the bonus chapter to learn how to considerably speed up your code!\nWe continue to work with the “Beat the blues” (BtheB) data set from the HSAUR R package. As there are multiple post-treatment measurements (after 2, 3, 6, and 8 months) we can create a nice longitudinal multilevel data set, with measurement points on level 1 (L1) and persons on level 2 (L2). We will work with the lme4 package to run the mixed-effect models.\nFor that, we first have to transform the pilot data set into the long format:\nHere we plot the trajectory of the first 20 participants. As you can see, there are missing values; not all participants have all follow-up values:"
  },
  {
    "objectID": "LMM.html#the-formulas",
    "href": "LMM.html#the-formulas",
    "title": "Ch. 4: Linear Mixed Models / Multilevel models",
    "section": "The formulas",
    "text": "The formulas\n\ni = index for time points\nj = index for persons\n\nLevel 1 equation:\n\n\\text{BDI}_{ij} = \\beta_{0j} + \\beta_{1j} time_{ij} + e_{ij}\n\nLevel 2 equations:\n\n\\beta_{0j} = \\gamma_{00} + u_{0j}\\\\\n\\beta_{1j} = \\gamma_{10}\n\nCombined equation:\n\n\\text{BDI}_{ij} = \\gamma_{00} + \\gamma_{10} time_{ij}  + u_{0j} + e_{ij} \\\\\n\\\\\ne_{ij} \\mathop{\\sim}\\limits^{\\mathrm{iid}} N(mean=0, var=\\sigma^2) \\\\\nu_{0j} \\mathop{\\sim}\\limits^{\\mathrm{iid}} N(mean=0, var=\\tau_{00})"
  },
  {
    "objectID": "LMM.html#analysis-in-pilot-data",
    "href": "LMM.html#analysis-in-pilot-data",
    "title": "Ch. 4: Linear Mixed Models / Multilevel models",
    "section": "Analysis in pilot data",
    "text": "Analysis in pilot data\nFirst, let’s run the analysis model in the pilot data. To make the intercept more interpretable, we center it on the first post-measurement by subtracting 2 (i.e., month “2” becomes month “0” etc.).\n\nBtheB_long$time.c <- BtheB_long$time - 2\n\nl0 <- lmer(BDI ~ 1 + time.c + (1|person_id), data=BtheB_long)\nsummary(l0)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: BDI ~ 1 + time.c + (1 | person_id)\n   Data: BtheB_long\n\nREML criterion at convergence: 1929.4\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.6308 -0.4698 -0.0810  0.3536  3.7142 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n person_id (Intercept) 97.15    9.857   \n Residual              25.48    5.048   \nNumber of obs: 280, groups:  person_id, 97\n\nFixed effects:\n            Estimate Std. Error       df t value Pr(>|t|)    \n(Intercept)  16.9691     1.0990 110.4143  15.441  < 2e-16 ***\ntime.c       -0.6869     0.1486 192.8764  -4.623 6.91e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n       (Intr)\ntime.c -0.267\n\n\nHence, in the pilot data there is a significant negative trend – with each month, participants have a decrease of 0.7 BDI points. But remember that in this data set all of the participants have been treated (with one of two treatments). For a passive control group we probably would expect no, or at least a much smaller negative trend over time. (There could be spontaneous remissions, but if this effect is assumed to be very strong, there would be no need for psychotherapy).\nThe grand intercept \\gamma_{00} is 17. This is also the average post-treatment value that we assumed in our previous models in Chapters 1 and 2.\nFor the random intercept variance and the residual variance, we take generously rounded estimates from the pilot data: \\tau_{00} = 100 and \\sigma^2 = 25."
  },
  {
    "objectID": "LMM.html#lets-simulate",
    "href": "LMM.html#lets-simulate",
    "title": "Ch. 4: Linear Mixed Models / Multilevel models",
    "section": "Let’s simulate",
    "text": "Let’s simulate\nThe next script shows how to simulate multilevel data with a random intercept. We first create a L2 data set (where each row is once participant). This contains a person id (which is necessary to merge the L1 and the L2 data set) and the random intercepts (i.e., the random deviations of persons from the fixed intercept). In more complex models, this also contains all L2 predictors.\nNext we create a L1 data set, and then merge both into one long format data set. We closely simulate the situation of the pilot data: All participants are treated and show a negative trend.\n\n#------  Setting the model parameters ---------------\n# between-person random intercept variance: var(u_0j) = tau_00\ntau_00 <- 100\n\ngamma_00 <- 17    # the grand (fixed) intercept\ngamma_10 <- -0.7  # the fixed slope for time\nresidual_var <- 25\n\nn_persons <- 100\n\n# Create a L2 (person-level) data set; each row is one person\ndf_L2 <- data.frame(\n  person_id = 1:n_persons,\n  u_0j = rnorm(n_persons, mean=0, sd=sqrt(tau_00))\n)\n\n# Create a L1 (measurement-point-level) data set; each row is one measurement\ndf_L1 <- data.frame(\n  person_id = rep(1:n_persons, each=4),  # create 4 rows for each person (as we have 4 measurements)  \n  time.c = rep(c(0, 2, 4, 6), times=n_persons)  \n)\n\n# combine to a long format data set  \ndf_long <- merge(df_L1, df_L2, by=\"person_id\")\n  \n# compute the DV by writing down the combined equation\ndf_long <- within(df_long, {\n  BDI = gamma_00 + gamma_10*time.c + u_0j + rnorm(n_persons*4, mean=0, sd=sqrt(residual_var))\n})\n\nl1 <- lmer(BDI ~ 1 + time.c + (1|person_id), data=df_long)\nsummary(l1)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: BDI ~ 1 + time.c + (1 | person_id)\n   Data: df_long\n\nREML criterion at convergence: 2681.8\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.87552 -0.58921 -0.04505  0.61462  2.50725 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n person_id (Intercept) 129.04   11.359  \n Residual               21.43    4.629  \nNumber of obs: 400, groups:  person_id, 100\n\nFixed effects:\n            Estimate Std. Error       df t value Pr(>|t|)    \n(Intercept)  16.5616     1.2001 113.5218  13.800  < 2e-16 ***\ntime.c       -0.7500     0.1035 299.0000  -7.246 3.66e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n       (Intr)\ntime.c -0.259\n\n\nNow we put this data generating script into a function, and run our power analysis:\n\n# Note: This code could be more optimized for speed, but this way is easier to understand\nsim4 <- function(n_persons = 100, tau_00 = 100, gamma_00 = 17, gamma_10 = -0.7, \n                 residual_var = 25, print=FALSE) {\n\n  df_L2 <- data.frame(\n    person_id = 1:n_persons,\n    u_0j = rnorm(n_persons, mean=0, sd=sqrt(tau_00))\n  )\n  \n  df_L1 <- data.frame(\n    person_id = rep(1:n_persons, each=4),  # create 4 rows for each person (as we have 4 measurements)\n    time.c = rep(c(0, 2, 4, 6), times=n_persons)  \n  )\n  \n  # combine to a long format data set  \n  df_long <- merge(df_L1, df_L2, by=\"person_id\")\n  \n  # simulate response variable, based on combined equation  \n  df_long <- within(df_long, {\n    BDI = gamma_00 + gamma_10*time.c + u_0j + rnorm(n_persons*4, mean=0, sd=sqrt(residual_var))\n  })\n  \n  # compute p-values\n  # these options might speed up code execution a little bit:\n  # , control=lmerControl(optimizer=\"bobyqa\", calc.derivs = FALSE)\n  l1 <- lmer(BDI ~ 1 + time.c + (1|person_id), data=df_long)\n  \n  if (print==TRUE) print(summary(l1))\n  else return(summary(l1)$coefficients[, \"Pr(>|t|)\"])\n}\n\n\nset.seed(0xBEEF)\niterations <- 1000\nns <- seq(30, 50, by=5)\n\nresult <- data.frame()\n\nfor (n_persons in ns) {\n  system.time({\n    p_values <- future_replicate(iterations, sim4(n_persons=n_persons), future.seed = TRUE)\n  })\n  \n  result <- rbind(result, data.frame(\n      n = n_persons,\n      power_intercept = sum(p_values[1, ] < .005)/iterations,\n      power_time.c    = sum(p_values[2, ] < .005)/iterations\n    )\n  )\n  \n  # show the result after each run (not shown here in the tutorial)\n  print(result)\n}\n\n\n\n   n power_intercept power_time.c\n1 30               1        0.700\n2 35               1        0.786\n3 40               1        0.874\n4 45               1        0.912\n5 50               1        0.937\n\n\nHence, we need around 36 persons to achieve a power of 80% to detect this slope."
  },
  {
    "objectID": "LMM.html#comparison-with-other-approaches",
    "href": "LMM.html#comparison-with-other-approaches",
    "title": "Ch. 4: Linear Mixed Models / Multilevel models",
    "section": "Comparison with other approaches",
    "text": "Comparison with other approaches\nMurayama and colleagues (2022) recently released an approximate but easy approach for LMM power analysis based on pilot data. They also provide an interactive Shiny app.\nGo to the tab “Level-1 predictor -> Planning L2 sample size only” and enter the values from the pilot study into the fields in the left panel:\n\n(Absolute) t value of the focal level-1 predictor = 4.623\nLevel-2 sample size = 97\nNumber of cross-level interactions related to the focal level-1 predictor = 0\n\nThis approximation yields very close values to our simulation:\n\n\n\nScreenshot from Murayama app\n\n\nAnother option is the app “PowerAnalysisIL” by Lafit (2021). In that app you would need to choose “Model 4: Effect of a level-1 continuous predictor (fixed slope)” and set “Autocorrelation of level-1 errors” to zero to estimate an equivalent model to ours. This app, however, is much slower than our own simulations, and cannot perfectly mirror our model, as the continuous L1 predictor cannot be defined exactly as we need it.\nNonetheless, the estimates are not far away from ours:\n\n\n\nScreenshot from Lafit app"
  },
  {
    "objectID": "LMM.html#the-formulas-1",
    "href": "LMM.html#the-formulas-1",
    "title": "Ch. 4: Linear Mixed Models / Multilevel models",
    "section": "The formulas",
    "text": "The formulas\n\ni = index for time points\nj = index for persons\n\nLevel 1 equation:\n\n\\text{BDI}_{ij} = \\beta_{0j} + \\beta_{1j} time_{ij} + e_{ij}\n\nLevel 2 equations:\n\n\\beta_{0j} = \\gamma_{00} + \\gamma_{01} \\text{treatment}_j + u_{0j}\\\\\n\\beta_{1j} = \\gamma_{10} + \\gamma_{11} \\text{treatment}_j + u_{1j}\n\nCombined equation:\n\n\\text{BDI}_{ij} = \\gamma_{00} + \\gamma_{01} \\text{treatment}_j + \\gamma_{10} time_{ij} +  \\gamma_{11} \\text{treatment}_j time_{ij} + u_{1j} time_{ij} + u_{0j} + e_{ij} \\\\\n\\\\\ne_{ij} \\mathop{\\sim}\\limits^{\\mathrm{iid}} N(mean=0, var=\\sigma^2) \\\\\nu_{0j} \\mathop{\\sim}\\limits^{\\mathrm{iid}} N(mean=0, var=\\tau_{00}) \\\\\nu_{1j} \\mathop{\\sim}\\limits^{\\mathrm{iid}} N(mean=0, var=\\tau_{11}) \\\\\ncov(u_{0j}, u_{1j}) = \\tau_{01}"
  },
  {
    "objectID": "LMM.html#setting-the-population-parameter-values",
    "href": "LMM.html#setting-the-population-parameter-values",
    "title": "Ch. 4: Linear Mixed Models / Multilevel models",
    "section": "Setting the population parameter values",
    "text": "Setting the population parameter values\nWe need to assume four additional population values, namely: \\tau_{11} (the variance of random slopes), \\tau_{01} (the intercept-slope-covariance), \\gamma_{01} (the treatment main effect), and \\gamma_{11} (the interaction effect).\nFixed effects:\n\nThe grand intercept \\gamma_{00} now is set to 23.\nThe treatment effect \\gamma_{01} is set to -6 (as before). In this more complex model, the treatment effect refers to a difference between treatment and control group directly after treatment.\nAs mentioned above, it is reasonable to assume that there should be no marked trend in a passive control group. Therefore, we set the conditional main effect for time, \\gamma_{10} to 0.\nFor the interaction effect, we expect a steeper decline in the treatment group (e.g., because the treatment “unfolds” its effect over time). If we set the interaction effect to -0.7, this translates to a predicted slope of -0.7 in the treatment group: \\gamma_{10} time_{ij} + \\gamma_{11} \\text{treatment}_j time_{ij} = (\\gamma_{10} + \\gamma_{11} \\text{treatment}_j) time_{ij}.\n\nIs this slope of -0.7, estimated from the pilot data, reasonable? Let’s extrapolate the trend: Treated patients have an average BDI score of 17 (directly after treatment). If they decline 0.7 points per month, they would have a BDI difference of 12*-0.7 = -8.4 after one year, and are at around 9 BDI points. With that decline, they would have moved from a mild depression to a minimal depression. This seems plausible.\nRandom terms:\n\nAssume that ~95% of slopes in the treatment group lie between -0.7 ± 0.3 (i.e., between -0.4 and -1). This corresponds to a standard deviation of 0.15, and consequently a variance of \\tau_{11} = 0.15^2 = 0.0225.\nWe assume no random effect correlation, so \\tau_{01} = 0.\n\nAs recommended in the last chapter, we visualize this assumed interaction effect by a hand drawing:\n\n\n\nhand-drawn interaction plot\n\n\nLooks good!"
  },
  {
    "objectID": "LMM.html#lets-simulate-1",
    "href": "LMM.html#lets-simulate-1",
    "title": "Ch. 4: Linear Mixed Models / Multilevel models",
    "section": "Let’s simulate",
    "text": "Let’s simulate\n\nsim5 <- function(n_persons = 100, tau_00 = 100, tau_11 = 0.0225, tau_01 = 0, \n                 gamma_00 = 23, gamma_01 = -6, gamma_10 = 0, gamma_11 = -0.7, \n                 residual_var = 25, print=FALSE) {\n\n  # create (correlated) random effect structure\n  mu <- c(0 ,0)\n  sigma <- matrix(\n    c(tau_00, tau_01, \n      tau_01, tau_11), nrow=2, byrow=TRUE)\n  RE <- rmvnorm(n=n_persons, mu=mu, sigma=sigma) |> data.frame()\n  names(RE) <- c(\"u_0j\", \"u_1j\")\n  \n  df_L1 <- data.frame(\n    person_id = 1:n_persons,\n    u_0j = RE$u_0j,\n    u_1j = RE$u_1j,\n    treatment = rep(c(0, 1), times=n_persons/2)\n  )\n  \n  df_L2 <- data.frame(\n    person_id = rep(1:n_persons, each=4),  # create 4 rows for each person (as we have 4 measurements)\n    time.c = rep(c(0, 2, 4, 6), times=n_persons)  \n  )\n  \n  # combine to a long format data set  \n  df_long <- merge(df_L1, df_L2, by=\"person_id\")\n  \n  # simulate response variable, based on combined equation  \n  df_long <- within(df_long, {\n      BDI = gamma_00 + # intercept\n      gamma_10*time.c + gamma_01*treatment + gamma_11*time.c*treatment +  # fixed terms\n      u_0j + u_1j*time.c + rnorm(n_persons*4, mean=0, sd=sqrt(residual_var))  # random terms\n  })\n  \n  # for debugging: plot some simulated participants\n  #ggplot(df_long[df_long$person_id <=20, ], aes(x=time.c, y=BDI, color=factor(treatment), group=person_id)) +\n  # geom_point() + geom_line() + facet_wrap(~person_id)\n  \n  # ggplot(df_long, aes(x=time.c, y=BDI, color=factor(treatment))) +\n  # stat_summary(fun.data=mean_cl_normal, geom = \"pointrange\")\n  \n  # compute p-values\n  # these options might speed up code execution a little bit:\n  # , control=lmerControl(optimizer=\"bobyqa\", calc.derivs = FALSE)\n  l1 <- lmer(BDI ~ 1 + time.c*treatment + (1 + time.c|person_id), data=df_long)\n  \n  if (print==TRUE) print(summary(l1))\n  else return(summary(l1)$coefficients[, \"Pr(>|t|)\"])\n}\n\n\nset.seed(0xBEEF)\niterations <- 1000\nns <- seq(60, 200, by=10) # must be dividable by 2\n\nresult <- data.frame()\n\nfor (n_persons in ns) {\n  system.time({\n    p_values <- future_replicate(iterations, sim5(n_persons=n_persons), future.seed = TRUE)\n  })\n  \n  result <- rbind(result, data.frame(\n      n = n_persons,\n      power_intercept = sum(p_values[1, ] < .005)/iterations,\n      power_time.c    = sum(p_values[2, ] < .005)/iterations,\n      power_treatment = sum(p_values[3, ] < .005)/iterations,\n      power_IA        = sum(p_values[4, ] < .005)/iterations\n    )\n  )\n  \n  # show the result after each run (not shown here in the tutorial)\n  print(result)\n}\n\n\n\n     n power_intercept power_time.c power_treatment power_IA\n1   60               1        0.004           0.267    0.306\n2   70               1        0.006           0.302    0.422\n3   80               1        0.002           0.413    0.439\n4   90               1        0.003           0.386    0.514\n5  100               1        0.002           0.449    0.563\n6  110               1        0.007           0.462    0.628\n7  120               1        0.007           0.596    0.702\n8  130               1        0.003           0.567    0.768\n9  140               1        0.005           0.629    0.797\n10 150               1        0.005           0.720    0.844\n11 160               1        0.006           0.768    0.835\n12 170               1        0.003           0.790    0.865\n13 180               1        0.005           0.787    0.888\n14 190               1        0.004           0.815    0.913\n15 200               1        0.006           0.904    0.937\n\n\nYou will see a lot of warnings about boundary (singular) fit – you can generally ignore these; they typically occur when one of the random variances is exactly zero; but the fixed effect estimates (which are our focus here) are still valid. You will also see some rare warnings about Model failed to converge with max|grad|. In this case, the optimizer did not converge with the required precision, but typically still is very close. Finally, there are some warnings Model failed to converge with 1 negative eigenvalue. These instances give wrong results. However, if in our thousands of simulations some very few models did not converge, this makes no noticeable difference. If, however, the majority of your models does not converge this is a hint that your simulation has some errors.\nConcerning the computed power, we see that we need around 180 participants for the treatment main effect (which mirrors the result from Chapter 1), and around 140 participants for detecting the interaction effect. As there is no main effect for time.c (it has been simulated as zero), the power stays always around the \\alpha-level."
  },
  {
    "objectID": "optimizing_code.html",
    "href": "optimizing_code.html",
    "title": "Bonus: Optimizing R code for speed",
    "section": "",
    "text": "Reading/working time: ~30 min.\nOptimizing code for speed can be an art – and you get lost and spend/waste hours by micro-optimizing some milliseconds. But the Pareto principle applies here: with 20% effort, you can have quick and substantial gains.\nCode profiling means that the code execution is timed, just like you had a stopwatch. Your goal is to make your code snippet as fast as possible. RStudio has a built-in profiler that (in theory) allows to see which code line takes up the longest time. But in my experience, if the computation of each single line is very short (and the duration mostly comes from the many repetitions), it is very inaccurate (i.e., the time spent is allocated to the wrong lines). Therefore, we’ll resort to the simplest way of timing code: We will measure overall execution time by wrapping our code in a system.time({ ... }) call. Longer code blocks need to be wrapped in curly braces {...}. The function returns multiple timings; the relevant number for us is the “elapsed” time. This is also called the “wall clock” time – the time you actually have to wait until computation finished."
  },
  {
    "objectID": "optimizing_code.html#preparation-wrap-the-simulation-in-a-function",
    "href": "optimizing_code.html#preparation-wrap-the-simulation-in-a-function",
    "title": "Bonus: Optimizing R code for speed",
    "section": "Preparation: Wrap the simulation in a function",
    "text": "Preparation: Wrap the simulation in a function\nThe first step does not really change a lot: We put the simulation code into a separate function that returns the quantity of interest (in our case: the focal p-value). Different settings of the simulation parameters, such as the sample size or the effect size, can be defined as parameters of the function.\nEvery single function call sim() now gives you one simulated p-value – try it out!\nWe then use the replicate function to run the sim function many times and to store the resulting p-values in a vector. Programming the simulation in such a functional style also has the nice side effect that you do not have to pre-allocate the results vector; this is automatically done by the replicate function.\n\n# Wrap the code for a single simulation into a function. It returns the quantity of interest.\nsim <- function(n=100) {\n  # the \"n\" is now taken from the function parameter \"n\"\n  x <- cbind(\n    rep(1, n),\n    c(rep(0, n/2), rep(1, n/2))\n  )\n  \n  y <- 23 - 3*x[, 2] + rnorm(n, mean=0, sd=sqrt(117))\n  mdl <- RcppArmadillo::fastLmPure(x, y)\n  p_val <- 2*pt(abs(mdl$coefficients[2]/mdl$stderr[2]), mdl$df.residual, lower.tail=FALSE)\n\n  return(p_val)\n}\n\nt5 <- system.time({\n\niterations <- 5000\nns <- seq(300, 500, by=50)\n\nresult <- data.frame()\n\nfor (n in ns) {\n  p_values <- replicate(n=iterations, sim(n=n))\n  result <- rbind(result, data.frame(n = n, power = sum(p_values < .005)/iterations))\n}\n\n})\ntimings <- rbind(t0[3], t1[3], t2[3], t3[3], t4[3], t5[3]) |> data.frame()\ntimings$diff <- c(NA, timings[2:nrow(timings), 1] - timings[1:(nrow(timings)-1), 1])\ntimings$rel_diff <- c(NA, timings[2:nrow(timings), \"diff\"]/timings[1:(nrow(timings)-1), 1]) |> round(3)\ntimings\n\n  elapsed   diff rel_diff\n1   9.888     NA       NA\n2  10.579  0.691    0.070\n3   7.384 -3.195   -0.302\n4   0.793 -6.591   -0.893\n5   0.764 -0.029   -0.037\n6   0.935  0.171    0.224\n\n\nWhile this refactoring actually slightly increased computation time, we need this for the last, final optimization where we reap the benefits."
  },
  {
    "objectID": "optimizing_code.html#run-on-multiple-cores",
    "href": "optimizing_code.html#run-on-multiple-cores",
    "title": "Bonus: Optimizing R code for speed",
    "section": "Run on multiple cores",
    "text": "Run on multiple cores\nWith the use of the replicate function in the previous step, we prepared everything for an easy switch to multi-core processing. You only need to load the future.apply package, start a multi-core session with the plan command, and replace the replicate function call with future_replicate.\n\n# Show how many cores are available on your machine:\navailableCores()\n\n# with plan() you enter the parallel mode. Enter the number of workers (aka. CPU cores)\nplan(multisession, workers = 4)\n\nt6 <- system.time({\n\niterations <- 5000\nns <- seq(300, 500, by=50)\n\nresult <- data.frame()\n\nfor (n in ns) {\n  # future.seed = TRUE is needed to set seeds in all parallel processes. Then the computation is reproducible.\n  p_values <- future_replicate(n=iterations, sim(n=n), future.seed = TRUE)\n  result <- rbind(result, data.frame(n = n, power = sum(p_values < .005)/iterations))\n}\n\n})\n\ntimings <- rbind(t0[3], t1[3], t2[3], t3[3], t4[3], t5[3], t6[3]) |> data.frame()\ntimings$diff <- c(NA, timings[2:nrow(timings), 1] - timings[1:(nrow(timings)-1), 1])\ntimings$rel_diff <- c(NA, timings[2:nrow(timings), \"diff\"]/timings[1:(nrow(timings)-1), 1]) |> round(3) |> round(2)\ntimings\n\n\n\n  elapsed   diff rel_diff\n1   9.888     NA       NA\n2  10.579  0.691     0.07\n3   7.384 -3.195    -0.30\n4   0.793 -6.591    -0.89\n5   0.764 -0.029    -0.04\n6   0.935  0.171     0.22\n7   0.788 -0.147    -0.16\n\n\nThe speed improvement seems only small – with 4 workers, one might expect that the computations only need 1/4th of the previous time. But parallel processing creates some overhead. For example, 4 separate R sessions need to be created and all packages, code (and sometimes data) need to be loaded in each session. Finally, all results must be collected and aggregated from all separate sessions. This can add up to substantial one-time costs. If your (single-core) computations only take a few seconds or less, parallel processing can even take longer."
  },
  {
    "objectID": "Regression_to_mean.html",
    "href": "Regression_to_mean.html",
    "title": "Regression to the mean in pre-post-designs",
    "section": "",
    "text": "BDI_{post} = b_0 + 1*BDI_{pre} + e\nBDI_{post} = b_0 + b_1*BDI_{pre} + b_2*treatment + e\nHence, the pre value simply is carried forward to the post value. We add random error, as participants of course go somewhat up and down, but there is no systematic trend.\n\n  library(Rfast)\n\nLade nötiges Paket: Rcpp\n\n\nLade nötiges Paket: RcppZiggurat\n\n  n <- 10000\n  pre_post_cor <- 0.9\n  mu <- c(23, 23)\n  sigma <- matrix(c(117, pre_post_cor*sqrt(117)*sqrt(117), pre_post_cor*sqrt(117)*sqrt(117), 117), nrow=2, byrow=TRUE)\n  BDI <- rmvnorm(n, mu, sigma) |> data.frame()\n  \n  \n  xi <- rnorm(n, mean=23, sd=sqrt(117))\n  pre <-  1*xi + rnorm(n, mean=0, sd=2)\n  post <- 1*xi + rnorm(n, mean=0, sd=2)\n  \n  cor(pre, post)\n\n[1] 0.9679785\n\n  colMeans(BDI)\n\n      X1       X2 \n22.92187 22.95114 \n\n  var(BDI)\n\n         X1       X2\nX1 118.0560 106.2575\nX2 106.2575 118.1335\n\n  names(BDI) <- c(\"pre\", \"post\")\n  BDI$id <- 1:nrow(BDI)\n  summary(lm(post~pre, BDI))\n\n\nCall:\nlm(formula = post ~ pre, data = BDI)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-16.0754  -3.2091   0.0313   3.2705  19.4456 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 2.320078   0.110740   20.95   <2e-16 ***\npre         0.900060   0.004366  206.17   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.743 on 9998 degrees of freedom\nMultiple R-squared:  0.8096,    Adjusted R-squared:  0.8096 \nF-statistic: 4.251e+04 on 1 and 9998 DF,  p-value: < 2.2e-16\n\n  library(tidyr)\n  library(ggplot2)\n  BDI_long <- pivot_longer(BDI, c(pre, post), names_to = \"time\")\n  ggplot(BDI_long, aes(x=time, y=value, group=id)) + geom_line()\n\n\n\n  # typically RTM pattern: The pre-post difference is\n  BDI$diff <- BDI$post-BDI$pre\n  BDI$absdiff <- abs(BDI$post-BDI$pre)\n  hist(BDI$diff)\n\n\n\n  mean(BDI$diff)\n\n[1] 0.02927231\n\n  summary(lm(diff~pre, BDI))\n\n\nCall:\nlm(formula = diff ~ pre, data = BDI)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-16.0754  -3.2091   0.0313   3.2705  19.4456 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  2.320078   0.110740   20.95   <2e-16 ***\npre         -0.099940   0.004366  -22.89   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.743 on 9998 degrees of freedom\nMultiple R-squared:  0.04981,   Adjusted R-squared:  0.04971 \nF-statistic: 524.1 on 1 and 9998 DF,  p-value: < 2.2e-16\n\n  ggplot(BDI, aes(x=pre, y=absdiff)) + geom_point() + geom_smooth()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n  ggplot(BDI, aes(x=pre, y=post)) + geom_point() + geom_smooth()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n  BDI$predicted <- predict(lm(post~pre, BDI))\n  mean(BDI$predicted)\n\n[1] 22.95114\n\n  var(BDI$predicted)\n\n[1] 95.63817\n\n  # The predicted values have the same mean (so no systematic treatment effect, as expected)\n  # but much smaller variance:\n  \n  BDI_long2 <- pivot_longer(BDI, c(pre, post, predicted), names_to = \"cat\")\n  \n  library(ggplot2)\nggplot(BDI_long2, aes(x=as.factor(cat), y=value)) + \n  ggdist::stat_halfeye(adjust = .5, width = .3, .width = 0, justification = -.3, point_colour = NA) + \n  geom_boxplot(width = .1, outlier.shape = NA) +\n  gghalves::geom_half_point(side = \"l\", range_scale = .4, alpha = .5)\n\n\n\n\nAssumed that we use the predicted scores at t2 (post) and predict the next scores at t3 – will it shrink ever more, until all data points are at the mean? But this is not substantively reflected in the raw scores. Is it an artifact of regression?"
  },
  {
    "objectID": "Resources.html",
    "href": "Resources.html",
    "title": "Resources",
    "section": "",
    "text": "SuperpowerBook by Aaron R. Caldwell, Daniël Lakens, Chelsea M. Parlett-Pelleriti, Guy Prochilo, and Frederik Aust. This teaches how to use the superpower R package to simulate factorial designs and calculate power.\nBlog post series “Power Analysis by Data Simulation in R” by Julian Quandt (see Parts I to IV). These blog posts provide a very detailed step-by-step explanation of the necessary steps.\n“Simulation-based power analysis for regression” by Andrew Hales (Youtube video, OSF Material). In this video, Andrew explains the steps to you.\nfaux package by Lisa DeBruine. Simulate data for factorial and mixed designs."
  },
  {
    "objectID": "SEM.html",
    "href": "SEM.html",
    "title": "Ch. 5: Structural Equation Models",
    "section": "",
    "text": "Reading/working time: ~50 min.\nIn this chapter, we will focus on some rather simple Structural Equation Models (SEM). The goal is to illustrate how simulations can be used to estimate statistical power to detect a given effect in a SEM. In the context of SEMs, the focal effect may be for instance a fit index (e.g., Chi-Square, Root Mean Square Error of Approximation, etc.) or a model coefficient (e.g., a regression coefficient for the association between two latent factors). In this chapter, we only focus on the latter, that is power analyses for regression coefficients in the context of SEM. Please see the bonus chapter titled “Power analysis for fit indices in SEM” if you’re interested in power analyses for fit indices."
  },
  {
    "objectID": "SEM.html#lets-get-some-real-data-as-starting-point",
    "href": "SEM.html#lets-get-some-real-data-as-starting-point",
    "title": "Ch. 5: Structural Equation Models",
    "section": "Let’s get some real data as starting point",
    "text": "Let’s get some real data as starting point\nJust like for any other simulation-based power analysis, we first need to come up with plausible estimates of the distribution of the (manifest) variables. For the sake of simplicity, let’s assume that there is a published study that measured manifestations of our two latent variables and that the corresponding data set is publicly available. For the purpose of this tutorial, we will draw on a publication by Bergh et al. (2016) and the corresponding data set which has been made accessible as part of the MPsychoR package. Let’s take a look at this data set.\n\n#install.packages(\"MPsychoR\")\nlibrary(MPsychoR)\n\ndata(\"Bergh\")\nattach(Bergh)\n\n#let's take a look\nhead(Bergh)\n\n        EP    SP  HP       DP     A1       A2       A3     O1       O2       O3\n1 2.666667 3.125 1.4 2.818182 3.4375 3.600000 3.352941 2.8750 3.400000 3.176471\n2 2.666667 3.250 1.4 2.545455 2.3125 2.666667 3.117647 4.4375 3.866667 4.470588\n3 1.000000 1.625 2.7 2.000000 3.5625 4.600000 3.941176 4.2500 3.666667 3.705882\n4 2.666667 2.750 1.8 2.818182 2.7500 3.200000 3.352941 2.8750 3.400000 3.117647\n5 2.888889 3.250 2.7 3.000000 3.2500 4.200000 3.764706 3.9375 4.400000 4.294118\n6 2.000000 2.375 1.7 2.181818 3.2500 3.333333 2.941176 3.8125 3.066667 3.411765\n  gender\n1   male\n2   male\n3   male\n4   male\n5   male\n6   male\n\ntail(Bergh)\n\n          EP    SP  HP       DP     A1       A2       A3     O1       O2\n856 2.000000 2.500 1.0 2.363636 3.3125 3.800000 3.705882 3.6875 3.733333\n857 1.555556 1.875 1.1 1.818182 2.6250 3.733333 3.000000 3.3125 2.800000\n858 3.000000 2.750 1.0 1.909091 3.3125 3.533333 3.882353 4.0000 3.533333\n859 1.444444 1.250 1.0 1.000000 3.8750 3.800000 3.529412 3.9375 3.533333\n860 1.222222 1.625 1.0 2.363636 2.8750 3.600000 3.411765 2.8125 3.600000\n861 1.555556 1.750 1.0 1.090909 3.3750 4.266667 4.058824 3.3750 2.800000\n          O3 gender\n856 4.411765 female\n857 3.529412 female\n858 3.823529 female\n859 3.411765 female\n860 3.058824 female\n861 3.588235 female\n\n\nThis data set comprises 11 variables measured in 861 participants. For now, we will focus on the following measured variables:\n\nEP is a continuous variable measuring ethnic prejudice.\nSP is a continuous variable measuring sexism.\nHP is a continuous variable measuring sexual prejudice towards gays and lesbians.\nDP is a continuous variable measuring prejudice toward people with disabilities.\nO1, O2, and O3 are three items measuring openness to experience.\n\nTo get an impression of this data, we look at the correlations of the variables we’re interested in.\n\ncor(cbind(EP, SP, HP, DP, O1, O2, O3)) |> round(2)\n\n      EP    SP    HP    DP    O1    O2    O3\nEP  1.00  0.53  0.25  0.53 -0.35 -0.36 -0.41\nSP  0.53  1.00  0.22  0.53 -0.33 -0.31 -0.33\nHP  0.25  0.22  1.00  0.24 -0.23 -0.30 -0.29\nDP  0.53  0.53  0.24  1.00 -0.30 -0.33 -0.34\nO1 -0.35 -0.33 -0.23 -0.30  1.00  0.66  0.74\nO2 -0.36 -0.31 -0.30 -0.33  0.66  1.00  0.71\nO3 -0.41 -0.33 -0.29 -0.34  0.74  0.71  1.00\n\n\nAs we have discussed in the previous chapters, the starting point of every simulation-based power analysis is to specify the population parameters of the variables of interest. In our example, we can estimate the population parameters from the study by Bergh et al. (2016). We start by calculating the means of the variables, rounding them generously, and storing them in a vector called means_vector.\n\n#store means\nmeans_vector <- c(mean(EP), mean(SP), mean(HP), mean(DP), mean(O1), mean(O2), mean(O3)) |> round(2)\n\n#Let's take a look\nmeans_vector\n\n[1] 1.99 2.11 1.22 2.06 3.55 3.49 3.61\n\n\nWe also need the variance-covariance matrix of our variables in order to simulate data. Luckily, we can estimate this from the Bergh et al. data as well. There are two ways to do this. First, we can use the cov function to obtain the variance-covariance matrix. This matrix incorporates the variance of each variable on the diagonal, and the covariances in the remaining cells.\n\n#store covariances\ncov_mat <- cov(cbind(EP, SP, HP, DP, O1, O2, O3)) |> round(2)\n\n#Let's take a look\ncov_mat\n\n      EP    SP    HP    DP    O1    O2    O3\nEP  0.51  0.26  0.28  0.20 -0.12 -0.12 -0.15\nSP  0.26  0.47  0.24  0.19 -0.11 -0.10 -0.12\nHP  0.28  0.24  2.44  0.20 -0.18 -0.22 -0.23\nDP  0.20  0.19  0.20  0.28 -0.08 -0.08 -0.09\nO1 -0.12 -0.11 -0.18 -0.08  0.23  0.15  0.18\nO2 -0.12 -0.10 -0.22 -0.08  0.15  0.22  0.17\nO3 -0.15 -0.12 -0.23 -0.09  0.18  0.17  0.26\n\n\nThis works well as long as we have a data set (e.g., from a pilot study or published work) to estimate the variances and covariances. In other cases, however, we might not have access to such a data set. In this case, we might only have a correlation table that was provided in a published paper. But that’s no problem either, as we can transform the correlations and standard deviations of the variables of interest into a variance-covariance matrix. The following chunk shows how this works by using the cor2cov function from the MBESS package.\n\n#store correlation matrix \ncor_mat <- cor(cbind(EP, SP, HP, DP, O1, O2, O3)) \n\n#store standard deviations\nsd_vector <- c(sd(EP), sd(SP), sd(HP), sd(DP), sd(O1), sd(O2), sd(O3))\n\n#transform correlations and standard deviations into variance-covariance matrix\ncov_mat2 <- MBESS::cor2cov(cor.mat = cor_mat, sd = sd_vector) |> as.data.frame() |> round(2)\n\n#Let's take a look\ncov_mat2\n\n      EP    SP    HP    DP    O1    O2    O3\nEP  0.51  0.26  0.28  0.20 -0.12 -0.12 -0.15\nSP  0.26  0.47  0.24  0.19 -0.11 -0.10 -0.12\nHP  0.28  0.24  2.44  0.20 -0.18 -0.22 -0.23\nDP  0.20  0.19  0.20  0.28 -0.08 -0.08 -0.09\nO1 -0.12 -0.11 -0.18 -0.08  0.23  0.15  0.18\nO2 -0.12 -0.10 -0.22 -0.08  0.15  0.22  0.17\nO3 -0.15 -0.12 -0.23 -0.09  0.18  0.17  0.26\n\n\nLet’s do a plausibility check: Did the two ways to estimate the variance-covariance matrix lead to the same results?\n\ncov_mat == cov_mat2\n\n     EP   SP   HP   DP   O1   O2   O3\nEP TRUE TRUE TRUE TRUE TRUE TRUE TRUE\nSP TRUE TRUE TRUE TRUE TRUE TRUE TRUE\nHP TRUE TRUE TRUE TRUE TRUE TRUE TRUE\nDP TRUE TRUE TRUE TRUE TRUE TRUE TRUE\nO1 TRUE TRUE TRUE TRUE TRUE TRUE TRUE\nO2 TRUE TRUE TRUE TRUE TRUE TRUE TRUE\nO3 TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n\n\nIndeed, this worked! Both procedures lead to the exact same variance-covariance matrix. Now that we have an approximation of the variance-covariance matrix, we use the rmvnorm function from the Rfast package to simulate data from a multivariate normal distribution. The following code simulates n = 50 observations from the specified population.\n\n#Set seed to make results reproducible\nset.seed(21364)\n\n#simulate data\nmy_first_simulated_data <- Rfast::rmvnorm(n = 50, mu=means_vector, sigma = cov_mat) |> as.data.frame()\n\n#Let's take a look\nhead(my_first_simulated_data)\n\n        EP        SP         HP       DP       O1       O2       O3\n1 1.175231 2.1563276 -0.4498951 1.535653 3.520299 3.768441 3.877645\n2 2.615520 1.7527931 -0.2546117 1.728349 3.048912 2.816513 2.743648\n3 1.849380 2.0383562 -0.3877381 2.055153 3.516697 3.354251 3.792305\n4 1.901085 2.5438523  2.0475777 2.150921 3.824178 3.631459 3.947051\n5 2.182503 2.1326429 -0.2088395 2.674773 3.948864 3.642066 4.005110\n6 1.691124 0.9746677  1.9193559 1.652397 4.184485 3.236144 3.669261\n\n\nWe could now fit a SEM to this simulated data set and check whether the regression coefficient modelling the association between openness to experience and generalized prejudice is significant at an \\alpha-level of .005. We will work with the lavaan package to fit SEMs.\n\n#specify SEM\nmodel_sem <- \"generalized_prejudice =~ EP + DP + SP + HP\n              openness =~ O1 + O2 + O3\n              generalized_prejudice ~ openness\"\n\n#fit the SEM to the simulated data set\nfit_sem <- sem(model_sem, data = my_first_simulated_data)\n\n#display the results\nsummary(fit_sem)\n\nlavaan 0.6.15 ended normally after 36 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        15\n\n  Number of observations                            50\n\nModel Test User Model:\n                                                      \n  Test statistic                                20.528\n  Degrees of freedom                                13\n  P-value (Chi-square)                           0.083\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                           Estimate  Std.Err  z-value  P(>|z|)\n  generalized_prejudice =~                                    \n    EP                        1.000                           \n    DP                        0.891    0.298    2.989    0.003\n    SP                        1.156    0.383    3.022    0.003\n    HP                        0.321    0.677    0.474    0.636\n  openness =~                                                 \n    O1                        1.000                           \n    O2                        0.857    0.155    5.544    0.000\n    O3                        1.370    0.225    6.086    0.000\n\nRegressions:\n                          Estimate  Std.Err  z-value  P(>|z|)\n  generalized_prejudice ~                                    \n    openness                -0.259    0.204   -1.270    0.204\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .EP                0.352    0.082    4.277    0.000\n   .DP                0.084    0.037    2.292    0.022\n   .SP                0.165    0.064    2.577    0.010\n   .HP                2.475    0.496    4.990    0.000\n   .O1                0.065    0.019    3.433    0.001\n   .O2                0.067    0.017    3.996    0.000\n   .O3                0.045    0.027    1.655    0.098\n   .generlzd_prjdc    0.138    0.078    1.766    0.077\n    openness          0.116    0.036    3.182    0.001\n\n\nThe results show that in this case, the regression coefficient is -0.26 which is significant with p = 0.204. But, actually, it is not our primary interest to see whether this particular simulated data set results in a significant regression coefficient. Rather, we want to know how many of a theoretically infinite number of simulations yield a significant p-value of the focal regression coefficient. Thus, as in the previous chapters, we now repeatedly simulate data sets of a certain size (say, 50 observations) from the specified population and store the results of the focal test (here: the p-value of the regression coefficient) in a vector called p_values.\n\n#Set seed to make results reproducible\nset.seed(21364)\n\n#let's do 500 iterations\niterations <- 500\n\n#prepare an empty NA vector with 500 slots\np_values <- rep(NA, iterations)\n\n#sample size per iteration\nn <- 50\n\n\n#simulate data\nfor(i in 1:iterations){\n\n  simulated_data <- Rfast::rmvnorm(n = n, mu = means_vector, sigma = cov_mat) |> as.data.frame()\n  fit_sem_simulated <- sem(model_sem, data = simulated_data)\n  \n  p_values[i] <- parameterestimates(fit_sem_simulated)[8,]$pvalue\n  \n}\n\nHow many of our 500 virtual samples would have found a significant p-value (i.e., p < .005)?\n\n#frequency table\ntable(p_values < .005)\n\n\nFALSE  TRUE \n  144   356 \n\n#percentage of significant results\nsum(p_values < .005)/iterations*100\n\n[1] 71.2\n\n\nOnly 71.2% of samples with the same size of n=50 result in a significant p-value. We conclude that n=50 observations seems to be insufficient, as the power with these parameters is lower than 80%."
  },
  {
    "objectID": "SEM.html#sample-size-planning-find-the-necessary-sample-size",
    "href": "SEM.html#sample-size-planning-find-the-necessary-sample-size",
    "title": "Ch. 5: Structural Equation Models",
    "section": "Sample size planning: Find the necessary sample size",
    "text": "Sample size planning: Find the necessary sample size\nBut how many observations do we need to find the presumed effect with a power of 80%? Like before, we can now systematically vary certain parameters (e.g., sample size) of our simulation and see how that affects power. We could, for example, vary the sample size in a range from 30 to 200. Running these simulations typically requires quite some computing time.\n\n#Set seed to make results reproducible\nset.seed(21364)\n\n#test ns between 30 and 200\nns_sem <- seq(30, 200, by=10) \n\n#prepare empty vector to store results\nresult_sem <- data.frame()\n\n#set number of iterations\niterations_sem <- 500\n\n#write function\nsim_sem <- function(n, model, mu, sigma) {\n  \n\n  simulated_data <- Rfast::rmvnorm(n = n, mu = mu, sigma = sigma) |> as.data.frame()\n  fit_sem_simulated <- sem(model, data = simulated_data)\n  p_value_sem <- parameterestimates(fit_sem_simulated)[8,]$pvalue\n  return(p_value_sem)\n  \n    }\n\n\n#replicate function with varying ns\nfor (n in ns_sem) {  \n  \np_values_sem <- future_replicate(iterations_sem, sim_sem(n = n, model = model_sem, mu = means_vector, sigma = cov_mat), future.seed=TRUE)  \nresult_sem <- rbind(result_sem, data.frame(\n    n = n,\n    power = sum(p_values_sem < .005)/iterations_sem)\n  )\n\n#The following line of code can be used to track the progress of the simulations \n#This can be helpful for simulations with a high number of iterations and/or a large parameter space which require a lot of time\n#I have deactivated this here; to enable it, just remove the \"#\" sign at the beginning of the next line\n#message(paste(\"Progress info: Simulations completed for n =\", n))\n\n\n}\n\nLet’s plot the results:\n\nggplot(result_sem, aes(x=n, y=power)) + geom_point() + geom_line() + scale_x_continuous(n.breaks = 18, limits = c(30,200)) + scale_y_continuous(n.breaks = 10, limits = c(0,1)) + geom_hline(yintercept= 0.8, color = \"red\")\n\n\n\n\nThis graph suggests that we need a sample size of approximately 50 participants to reach a power of 80% with the given population estimates. That’s all it takes to run a power analysis for a SEM!\nIn the following two sub-paragraphs, we would like to present two alternatives and/or extensions to this first way of doing a power analysis for a SEM. First, we would like to present an alternative way to simulate data for SEMs, i.e. by using a built-in function in the lavaan package. Second, we would like to show how a “safeguard” approach to power analysis can be used within the context of SEM."
  },
  {
    "objectID": "SEM.html#using-the-lavaan-syntax-to-simulate-data",
    "href": "SEM.html#using-the-lavaan-syntax-to-simulate-data",
    "title": "Ch. 5: Structural Equation Models",
    "section": "Using the lavaan syntax to simulate data",
    "text": "Using the lavaan syntax to simulate data\nIn our opening example in this chapter, we used the rmvnorm function from the Rfastpackage to simulate data based on the means of the manifest variables as well as their variance-covariance matrix. An alternative to this procedure is to use a built-in function in the lavaan package, that is, the simulateData() function. The main idea here is to provide this function with a lavaan model that specifies all relevant population parameters and then to use this function to directly simulate data.\nMore specifically, we need to incorporate the factor loadings, regression coefficients and (residual) variances of all latent and manifest variables in the lavaan syntax. As these parameters are hardly ever known without a previous study, we will again draw on the results from the data by Bergh et al. (2016). Let’s again take a look at the results of our SEM when applying it to this data set.\n\n#fit the SEM to the pilot data set\nfit_bergh <- sem(model_sem, data = Bergh)\n\n#display the results\nsummary(fit_bergh)\n\nlavaan 0.6.15 ended normally after 40 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        15\n\n  Number of observations                           861\n\nModel Test User Model:\n                                                      \n  Test statistic                                40.574\n  Degrees of freedom                                13\n  P-value (Chi-square)                           0.000\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                           Estimate  Std.Err  z-value  P(>|z|)\n  generalized_prejudice =~                                    \n    EP                        1.000                           \n    DP                        0.710    0.041   17.383    0.000\n    SP                        0.907    0.052   17.337    0.000\n    HP                        1.042    0.112    9.267    0.000\n  openness =~                                                 \n    O1                        1.000                           \n    O2                        0.932    0.035   26.255    0.000\n    O3                        1.143    0.040   28.923    0.000\n\nRegressions:\n                          Estimate  Std.Err  z-value  P(>|z|)\n  generalized_prejudice ~                                    \n    openness                -0.766    0.056  -13.641    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .EP                0.219    0.016   13.403    0.000\n   .DP                0.141    0.009   14.972    0.000\n   .SP                0.233    0.015   15.079    0.000\n   .HP                2.124    0.106   19.965    0.000\n   .O1                0.073    0.005   14.423    0.000\n   .O2                0.079    0.005   15.805    0.000\n   .O3                0.052    0.005    9.823    0.000\n   .generlzd_prjdc    0.191    0.018   10.362    0.000\n    openness          0.161    0.011   14.210    0.000\n\n\nIn order to use the simulateData() function, we can take the estimates from this previous study and plug them into the lavaan syntax in the following chunk.\n\n#Set seed to make results reproducible\nset.seed(21364)\n\n#specify SEM\nmodel_fully_specified <- \n\n\"generalized_prejudice =~ 1*EP + 0.71*DP + 0.91*SP + 1*HP\nopenness =~ 1*O1 + 0.93*O2 + 1.14*O3\ngeneralized_prejudice ~ -0.77*openness\n\n\ngeneralized_prejudice ~~ 0.19*generalized_prejudice\nopenness ~~ 0.16*openness\nEP ~~ 0.21*EP\nDP ~~ 0.14*DP\nSP ~~ 0.23*SP\nHP ~~ 2.12*HP\nO1 ~~ 0.07*O1\nO2 ~~ 0.08*O2\nO3 ~~ 0.05*O3\n\n\"\n\n#lets try this\nsim_lavaan <- simulateData(model_fully_specified, sample.nobs=100)\nhead(sim_lavaan)\n\n            EP         DP          SP         HP          O1         O2\n1  0.009175975 -0.4818076 -0.29785829 -3.5273250 -0.02254289 -0.1394437\n2 -0.204483904 -0.4787598  0.15013918 -3.1894504 -0.56917823 -0.2076622\n3  0.456774325 -0.6544974  0.05802164 -2.3337546  0.26092341  0.7767911\n4 -1.153264826 -0.3216415 -0.91603659 -2.0334785 -0.16649019 -0.1387532\n5  0.512683482  0.3138148  0.28465681  0.1202821 -0.41602783 -0.2699893\n6 -1.064347791 -0.3287166 -0.62215235 -2.4007914  0.60927732 -0.1814121\n           O3\n1 -0.22583796\n2 -1.17504320\n3  0.11402119\n4  0.06726909\n5 -0.62531893\n6  0.13393737\n\n\nThe next step is to integrate this code into our first simulation-based power analysis in this chapter, that is, to replace the data simulation process using the rmvnorm function from the Rfast package with this new method using simulateData() from the lavaan package. To this end, I am adapting the power analysis SEM chunk from above accordingly.\n\n#Set seed to make results reproducible\nset.seed(21364)\n\n#test ns between 30 and 200\nns_sem <- seq(30, 200, by=10) \n\n#prepare empty vector to store results\nresult_sem <- data.frame()\n\n#set number of iterations\niterations_sem <- 500\n\n#write function\nsim_sem_lavaan <- function(n, model) {\n  \n  \n  sim_lavaan <- simulateData(model = model, sample.nobs=n)\n  fit_sem_simulated <- sem(model_sem, data = sim_lavaan)\n  p_value_sem <- parameterestimates(fit_sem_simulated)[8,]$pvalue\n  return(p_value_sem)\n  \n}\n\n\n#replicate function with varying ns\nfor (n in ns_sem) {  \n  \n  p_values_sem <- future_replicate(iterations_sem, sim_sem_lavaan(n = n, model = model_fully_specified), future.seed=TRUE)  \n  result_sem <- rbind(result_sem, data.frame(\n    n = n,\n    power = sum(p_values_sem < .005, na.rm = TRUE)/iterations_sem)\n  )\n  \n#The following line of code can be used to track the progress of the simulations \n#This can be helpful for simulations with a high number of iterations and/or a large parameter space which require a lot of time\n#I have deactivated this here; to enable it, just remove the \"#\" sign at the beginning of the next line\n#message(paste(\"Progress info: Simulations completed for n =\", n))\n\n}\n\nLet’s plot this again.\n\nggplot(result_sem, aes(x=n, y=power)) + geom_point() + geom_line() + scale_x_continuous(n.breaks = 18, limits = c(30,200)) + scale_y_continuous(n.breaks = 10, limits = c(0,1)) + geom_hline(yintercept= 0.8, color = \"red\")\n\n\n\n\nHere, we conclude that approx. 55 participants would be needed to achieve 80% power. The slight difference compared to the previous power analysis should be explained by the fact that we rounded the numbers that define our statistical populations and that we only used 500 Monte Carlo iterations – these differences should decrease with an increasing number of iterations.\n\n\n\n\n\n\nTip\n\n\n\nWang and Rhemtulla (2021) developed a shiny app that can do power analyses for SEMs in a similar fashion, but that additionally provides a point-and-click interface. You can use it here, for instance, to replicate the results of this simulation-based power analysis: https://yilinandrewang.shinyapps.io/pwrSEM/"
  },
  {
    "objectID": "SEM.html#a-safeguard-power-approach-for-sems",
    "href": "SEM.html#a-safeguard-power-approach-for-sems",
    "title": "Ch. 5: Structural Equation Models",
    "section": "A safeguard power approach for SEMs",
    "text": "A safeguard power approach for SEMs\nOf note, the two power analyses for our SEM we have conducted so far used the observed effect size from the Bergh et al. (2016) data set as an estimate of the “true” effect size. But, as this point estimate may be imprecise (e.g., because of publication bias), it seems reasonable to use a more conservative estimate of the true effect size. One more conservative approach in this context is the safeguard power approach (Perugini et al., 2014), which we have already applied in Chapter 1 (Linear Model I: a single dichotomous predictor).\nBasically, all need to do in order to account for variability of observed effect sizes is to calculate a 60%-confidence interval around the point estimate of the observed effect size from our pilot data and to use the more conservative bound of this confidence interval (here: the upper bound) as our new effect size estimate. This can be easily done with the parameterestimates function from the lavaan package which takes the level of the confidence interval as an input parameter. Let’s use this function on our object fit_bergh which stores the results of our SEM in the Bergh data set.\n\nparameterestimates(fit_bergh, level = .60)\n\n                     lhs op                   rhs    est    se       z pvalue\n1  generalized_prejudice =~                    EP  1.000 0.000      NA     NA\n2  generalized_prejudice =~                    DP  0.710 0.041  17.383      0\n3  generalized_prejudice =~                    SP  0.907 0.052  17.337      0\n4  generalized_prejudice =~                    HP  1.042 0.112   9.267      0\n5               openness =~                    O1  1.000 0.000      NA     NA\n6               openness =~                    O2  0.932 0.035  26.255      0\n7               openness =~                    O3  1.143 0.040  28.923      0\n8  generalized_prejudice  ~              openness -0.766 0.056 -13.641      0\n9                     EP ~~                    EP  0.219 0.016  13.403      0\n10                    DP ~~                    DP  0.141 0.009  14.972      0\n11                    SP ~~                    SP  0.233 0.015  15.079      0\n12                    HP ~~                    HP  2.124 0.106  19.965      0\n13                    O1 ~~                    O1  0.073 0.005  14.423      0\n14                    O2 ~~                    O2  0.079 0.005  15.805      0\n15                    O3 ~~                    O3  0.052 0.005   9.823      0\n16 generalized_prejudice ~~ generalized_prejudice  0.191 0.018  10.362      0\n17              openness ~~              openness  0.161 0.011  14.210      0\n   ci.lower ci.upper\n1     1.000    1.000\n2     0.676    0.744\n3     0.863    0.951\n4     0.948    1.137\n5     1.000    1.000\n6     0.902    0.962\n7     1.110    1.176\n8    -0.814   -0.719\n9     0.205    0.233\n10    0.133    0.148\n11    0.220    0.246\n12    2.034    2.213\n13    0.069    0.078\n14    0.074    0.083\n15    0.048    0.057\n16    0.175    0.206\n17    0.152    0.171\n\n\nThis output shows that the upper bound of the 60% confidence interval around the focal regression coefficient is -0.72. We can now use this as our new and more conservative effect size estimate. We can for example insert this value into our simulation using the lavaan syntax. For this purpose, we copy the chunk from above and simply replace the previous effect size estimate (-0.77) with the new estimate (-0.72), while keeping all other parameters that define this data set.\n\n#Set seed to make results reproducible\nset.seed(21364)\n\n#specify SEM\nmodel_fully_specified_safeguard <- \n\n\"generalized_prejudice =~ 1*EP + 0.71*DP + 0.91*SP + 1*HP\nopenness =~ 1*O1 + 0.93*O2 + 1.14*O3\ngeneralized_prejudice ~ -0.72*openness\n\n\ngeneralized_prejudice ~~ 0.19*generalized_prejudice\nopenness ~~ 0.16*openness\nEP ~~ 0.21*EP\nDP ~~ 0.14*DP\nSP ~~ 0.23*SP\nHP ~~ 2.12*HP\nO1 ~~ 0.07*O1\nO2 ~~ 0.08*O2\nO3 ~~ 0.05*O3\n\n\"\n\n#lets try this\nsim_lavaan_safeguard <- simulateData(model_fully_specified_safeguard, sample.nobs=100)\nhead(sim_lavaan_safeguard)\n\n           EP         DP         SP         HP          O1         O2\n1  0.03163582 -0.4674398 -0.2762416 -3.5075535 -0.04391145 -0.1593227\n2 -0.19674199 -0.4764434  0.1605740 -3.1573002 -0.58997086 -0.2268869\n3  0.48315832 -0.6351419  0.0799879 -2.3272372  0.25229795  0.7683784\n4 -1.13893054 -0.3131634 -0.9001980 -2.0386232 -0.18372798 -0.1543441\n5  0.49985196  0.3039113  0.2740270  0.1386227 -0.41638579 -0.2703941\n6 -1.04098556 -0.3130325 -0.5996895 -2.4067345  0.59293247 -0.1965773\n           O3\n1 -0.25024294\n2 -1.19986667\n3  0.10431698\n4  0.04700113\n5 -0.62587386\n6  0.11528181\n\n\nNow, everything is ready for the actual safeguard power analysis. We can re-use the sim_sem_lavaan function we have defined above. Let’s see what we get here!\n\n#Set seed to make results reproducible\nset.seed(21364)\n\n#prepare empty vector to store results\nresult_sem_safeguard <- data.frame()\n\n#replicate function with varying ns\nfor (n in ns_sem) {  \n  \n  p_values_sem_safeguard <- future_replicate(iterations_sem, sim_sem_lavaan(n = n, model = model_fully_specified_safeguard), future.seed=TRUE)  \n  result_sem_safeguard <- rbind(result_sem_safeguard, data.frame(\n    n = n,\n    power = sum(p_values_sem_safeguard < .005, na.rm = TRUE)/iterations_sem)\n  )\n  \n#The following line of code can be used to track the progress of the simulations \n#This can be helpful for simulations with a high number of iterations and/or a large parameter space which require a lot of time\n#I have deactivated this here; to enable it, just remove the \"#\" sign at the beginning of the next line\n#message(paste(\"Progress info: Simulations completed for n =\", n))\n  \n}\n\nggplot(result_sem_safeguard, aes(x=n, y=power)) + geom_point() + geom_line() + scale_x_continuous(n.breaks = 18, limits = c(30,200)) + scale_y_continuous(n.breaks = 10, limits = c(0,1)) + geom_hline(yintercept= 0.8, color = \"red\")\n\n\n\n\nThis safeguard power analysis yields a required sample size of ca. 63 participants.\n\n\n\n\n\n\nNote\n\n\n\nIn addition to this safeguard power approach, we would also have liked to derive a smallest effect size of interest (SESOI). However, no prior studies on SESOI in the context of personality/stereotypes were available and the measures/response scales used by Bergh et al. (2016) were only vaguely reported in their manuscript, thereby making it difficult to derive a meaningful SESOI. We therefore only report a safeguard approach but no SESOI approach here."
  },
  {
    "objectID": "SEM_fit_index.html",
    "href": "SEM_fit_index.html",
    "title": "Bonus: Fit indices in SEM",
    "section": "",
    "text": "#install.packages(c(\"future.apply\", \"ggplot2\", \"lavaan\", \"MPsychoR\"), dependencies = TRUE)\n\nrequire(future.apply)\nrequire(ggplot2)\nrequire(lavaan)\nrequire(MPsychoR)\n\nIn this chapter, we will turn to simulation-based power analysis for fit indices in the context of SEM. We will build on the model we have introduced in Chapter 5 (Structural Equation Modelling (SEM)), it is therefore recommendable to read this chapter first. The following chunk specifies this model, in which (latent) generalized prejudice is predicted by (latent) openness to experience (as conceptualized in the big five personality traits). We fit this model to the dataset by Bergh et al. (2016) in order to get a first impression of the model fit.\n\ndata(\"Bergh\")\n\nmodel_sem <- \"generalized_prejudice =~ EP + DP + SP + HP\n              openness =~ O1 + O2 + O3\n              generalized_prejudice ~ openness\"\n\n#fit the SEM to the pilot data set\nfit <- sem(model_sem, data = Bergh)\n\nsummary(fit, fit.measures = TRUE)\n\nlavaan 0.6.15 ended normally after 40 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        15\n\n  Number of observations                           861\n\nModel Test User Model:\n                                                      \n  Test statistic                                40.574\n  Degrees of freedom                                13\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              2395.448\n  Degrees of freedom                                21\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.988\n  Tucker-Lewis Index (TLI)                       0.981\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -4740.818\n  Loglikelihood unrestricted model (H1)      -4720.530\n                                                      \n  Akaike (AIC)                                9511.635\n  Bayesian (BIC)                              9583.007\n  Sample-size adjusted Bayesian (SABIC)       9535.371\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.050\n  90 Percent confidence interval - lower         0.033\n  90 Percent confidence interval - upper         0.067\n  P-value H_0: RMSEA <= 0.050                    0.482\n  P-value H_0: RMSEA >= 0.080                    0.002\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.038\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                           Estimate  Std.Err  z-value  P(>|z|)\n  generalized_prejudice =~                                    \n    EP                        1.000                           \n    DP                        0.710    0.041   17.383    0.000\n    SP                        0.907    0.052   17.337    0.000\n    HP                        1.042    0.112    9.267    0.000\n  openness =~                                                 \n    O1                        1.000                           \n    O2                        0.932    0.035   26.255    0.000\n    O3                        1.143    0.040   28.923    0.000\n\nRegressions:\n                          Estimate  Std.Err  z-value  P(>|z|)\n  generalized_prejudice ~                                    \n    openness                -0.766    0.056  -13.641    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .EP                0.219    0.016   13.403    0.000\n   .DP                0.141    0.009   14.972    0.000\n   .SP                0.233    0.015   15.079    0.000\n   .HP                2.124    0.106   19.965    0.000\n   .O1                0.073    0.005   14.423    0.000\n   .O2                0.079    0.005   15.805    0.000\n   .O3                0.052    0.005    9.823    0.000\n   .generlzd_prjdc    0.191    0.018   10.362    0.000\n    openness          0.161    0.011   14.210    0.000\n\n\nThere are many different fit indices displayed in this output, for example the Comparative Fit Index (CFI), the Root Mean Square Error of Approximation (RMSEA) and the Standardized Root Mean Square Residual (SRMR). We can not go into the details of the interpretations of the fit indices here, but it is important to know that many of these indices are not very sensitive to sample size. Therefore, running a power analysis for these fit indices is not really meaningful. But instead of analyzing how one of these indices varies as a function of sample size, we can optimize the precision of one of these indices. For example, the lavaan output from above displays the 90% confidence interval for the RMSEA index. We could, for example, plan to find a certain sample size that ensures that the confidence interval around the RMSEA estimate has a certain maximum size, that is, the RMSEA estimate is sufficiently precise. This what we will learn to do in this chapter.\nAs a starting point, we again define the population parameters (i.e., the means, variances, and co-variances of all measured variables). We use the study by Bergh et al. (2016) to estimate these parameters (just as we did in Chapter 5).\n\nattach(Bergh)\n\n#store means\nmeans_vector <- c(mean(EP), mean(SP), mean(HP), mean(DP), mean(O1), mean(O2), mean(O3)) |> round(2)\n\n#store covariances\ncov_mat <- cov(cbind(EP, SP, HP, DP, O1, O2, O3)) |> round(2)\n\nWith these parameters, we can simulate data using the rmvnorm function from the Rfast package. The only difference to the simulation described in Chapter 5 is that here, we do not calculate and store the p-value of the regression coefficient, but rather, we compute the width of the RMSEA confidence interval and store it in a vector. We then count the number of simulations that yield a confidence interval with a maximum size of, say, .10. The next chunk shows how this is done.\n\nset.seed(9875234)\n\n#test ns between 50 and 200\nns <- seq(50, 200, by=10) \n\n#prepare empty vector to store results\nresult <- data.frame()\n\n#set number of iterations\niterations <- 1000\n\n#write function\nsim_sem <- function(n, model, mu, sigma) {\n  \n\n  simulated_data <- Rfast::rmvnorm(n = n, mu = mu, sigma = sigma) |> as.data.frame()\n  fit_sem_simulated <- sem(model_sem, data = simulated_data)\n  rmsea_ci_width <- as.numeric(fitMeasures(fit_sem_simulated)[\"rmsea.ci.upper\"] - fitMeasures(fit_sem_simulated)[\"rmsea.ci.lower\"])\n  return(rmsea_ci_width)\n  \n    }\n\n\n#replicate function with varying ns\nfor (n in ns) {  \n  \nrmsea_ci_width <- future_replicate(iterations, sim_sem(n = n, model = model_sem, mu = means_vector, sigma = cov_mat), future.seed=TRUE)  \nresult <- rbind(result, data.frame(\n    n = n,\n    power = sum(rmsea_ci_width < .1)/iterations)\n  )\n\n}\n\nLet’s plot this.\n\nggplot(result, aes(x=n, y=power)) + geom_point() + geom_line() + scale_x_continuous(n.breaks = 18, limits = c(30,200)) + scale_y_continuous(n.breaks = 10, limits = c(0,1)) + geom_hline(yintercept= 0.8, color = \"red\")\n\n\n\n\nThis analysis suggests that approx. 168 participants are needed to obtain a 90%-confidence interval around the RMSEA coefficient that is not larger than .10 in 80% of the cases."
  }
]